{
  "MainState": {
    "code": "\"\"\" (description)\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nfrom .State import *\nfrom builtins import *\n\nclass MainState(State):\n    _state = {\n        \"fraglen\": \"uint\",\n        \"refe\": \"StructureRepresentation\", # The reference structure\n        \"fraglib\": \"FragmentLibrary\",\n        \"nfrags\": \"uint\", # Here: number of fragments to describe the trajectory\n        \"bb_atoms\": \"ListOf(str)\",\n        \"stages\": \"ListOf(Stage)\",\n        \"downstream_best_score\": \"ListOf(float)\", # the best score that was found for fragment N until the end\n    }\n\n################################################################\n# Structure representation (of reference structure or fragment library)\n################################################################\n\ndef validate_sequence(seq: str,  state: \"State\"):\n    for aa in seq:\n        assert aa in \"ACDEFGHIKLMNPQRSTVWXYZ\", aa\n\nclass StructureRepresentation(State):\n    _state = {\n        \"coor\": (\"ndarray\", validate_coor_dtype),\n        \"sequence\": (\"str\", validate_sequence),\n        \"coor_fragment\": \"FragmentCoordinateRepresentation\",\n        \"coor_residue\": \"CoordinateRepresentation\",\n        \"nresidues\": \"uint\",\n        \"fraglen\": \"uint\",\n        \"nfrags\": \"uint\", # might be inherited from parent instead\n        \"bb_atoms\": \"ListOf(str)\",\n    }\n\nclass CenteredState(State):\n    def _check_centered(self):\n        for attr in self._state:\n            if not attr.endswith(\"_centered\"):\n                continue\n            value = getattr(self, attr)\n            if value is None:\n                continue\n            value2 = value.reshape(-1, 4)[:, :3]\n            com = value2.mean(axis=0)\n            if not np.allclose(com, np.zeros(3)):\n                raise ValueError(\"Not centered\", attr, com)        \n    def _validate(self):\n        self._check_centered()\n\nclass FragmentCoordinateRepresentation(CenteredState):\n    \"\"\"Note that different representations may not have the same center-of-mass!\"\"\"\n    _state = {\n        # From parent:\n        # \"nfrags\": \"uint\",   \n        # \"fraglen\": \"uint\",\n        # \"bb_atoms\": \"ListOf(str)\",\n\n        \"backbone\": (\"ndarray\", \"(nfrags, fraglen, len(bb_atoms), 3)\"),\n        \"backbone4\": (\"ndarray\", \"(nfrags, fraglen, len(bb_atoms), 4)\"),\n        \"backbone4_centered\": (\"ndarray\", \"(nfrags, fraglen, len(bb_atoms), 4)\"),\n        \"backbone_residuals\": (\"ndarray\", \"(nfrags,)\"),\n        \"backbone_com\": (\"ndarray\", \"(nfrags, 3)\"),\n        \"ca\": (\"ndarray\", \"(nfrags, fraglen, 3)\"),\n        \"ca4\": (\"ndarray\", \"(nfrags, fraglen, 4)\"),\n        \"ca4_centered\": (\"ndarray\", \"(nfrags, fraglen, 4)\"),\n        \"ca_residuals\": (\"ndarray\", \"(nfrags,)\"),\n        \"ca_com\": (\"ndarray\", \"(nfrags, 3)\"),\n    }\n\nclass CoordinateRepresentation(CenteredState):\n    \"\"\"Note that different representations may not have the same center-of-mass!\"\"\"\n    _state = {\n        \"backbone\": (\"ndarray\", \"(nresidues, len(bb_atoms), 3)\"),\n        \"backbone4\": (\"ndarray\", \"(nresidues, len(bb_atoms), 4)\"),\n        \"backbone4_centered\": (\"ndarray\", \"(nresidues, len(bb_atoms), 4)\"),\n        \"backbone_residual\": \"float\",\n        \"ca\": (\"ndarray\", \"(nresidues, 3)\"),\n        \"ca4\": (\"ndarray\", \"(nresidues, 4)\"),\n        \"ca4_centered\": (\"ndarray\", \"(nresidues, 4)\"),\n        \"ca_residual\": \"float\",\n    }\n\n################################################################\n# Fragment library \n################################################################\n\nclass FragmentLibrary(CenteredState):\n    _state = {\n        \"nfrags\": \"uint\",  # Here, the size of the fragment library\n        \"fraglen\": \"uint\",\n        \"bb_atoms\": \"ListOf(str)\",\n        \"coor\": \"FragmentCoordinateRepresentation\",\n        \"matrices\":(\"ndarray\", \"(nfrags,nfrags,4,4)\"),\n    }\n\n################################################################\n# Stages of growing the trajectory \n################################################################\n\nclass Stage(State):\n    _state = {\n        \"size\": \"uint\",      # current number of trajectories that this stage holds\n        \"maxsize\": \"uint\",  # maximum number of trajectories that this stage can hold\n\n        \"fragindex\": \"uint\",  # Here: number of fragments at the current stage\n        \"trajectories\": (\"ndarray\", \"(-1,fragindex)\"),  #16 bit unsigned int\n        # unsigned integer; trajectories of all fragments until now\n\n        \"matrices\": (\"ndarray\", \"(-1,4,4)\"),\n        \"scores\": (\"ndarray\", \"(-1,)\"),  # lower means better\n        \"score_threshold\": \"float\",  # reject all scores higher than threshold\n\n        # If/as long as we need to store coordinates, rather than matrices\n        \"nfrags\": \"uint\",  # Here, the same as maxsize\n        \"coor\": \"FragmentCoordinateRepresentation\",  #inherits nfrags from here\n\n        # If RMSD calculation to a reference\n        \"covar\": (\"ndarray\", \"(-1,3,3)\"),\n        \"residuals\": (\"ndarray\", \"(-1,)\"),\n        \"fragcoms\": (\"ndarray\", \"(-1, fragindex, 3)\"),\n        \n        # If we need to store *all* matrices (e.g. for forcefield calculations)\n        # We could also re-compute these at any time from the trajectories\n        \"all_matrices\": (\"ndarray\", \"(-1,fragindex,4,4)\"),\n    }",
    "dependencies": [
      ".State"
    ],
    "language": "python"
  },
  "State": {
    "code": "\"\"\" (description)\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nimport sys\nimport numpy as np\nfrom numpy.lib.arraysetops import isin\n\nBASICTYPES = (dict, list, tuple, int, float, bool, str, np.number)\n\nfrom numpy import uint32 as uint\n\ndefault_validators = {}\n\nndarray = np.ndarray\n\nclass ShapeError(TypeError):\n    pass\n\ndef validate_coor_dtype(arr: ndarray, state: \"State\") -> None:    \n    from .functions.parse_pdb import atomic_dtype\n    if arr.ndim != 1:\n        raise ShapeError(arr.shape)\n    if arr.dtype is not atomic_dtype:\n        raise TypeError(arr.dtype)\n\n\ndef ndarray_shape_validator(arr: ndarray, state: \"State\", shape_expr: str) -> None:\n    shape = state._eval_in_scope(shape_expr, evaluate_strings=True)\n    if len(shape) != len(arr.shape):\n        raise ShapeError(shape, arr.shape)\n    for x, y in zip(shape, arr.shape):\n        if x == -1:\n            continue\n        if x != y:\n            raise ShapeError(shape, arr.shape)\n\n\ndefault_validators[\"ndarray\"] = ndarray_shape_validator\n\nclass TypedList(list):\n    _type = None\n    def __init__(self, value=[]):\n        for v in value:\n            if not isinstance(v, self._type):\n                raise TypeError(v)\n        super().__init__(value)\n\ndef ListOf(x):\n    return type(\"ListOf({})\".format(x), (TypedList,), {\"_type\": x})\n\nclass _StateScope(dict):\n    def __init__(self, state: \"State\"):\n        self.state = state\n    def __getitem__(self, attr):\n        try:\n            return getattr(self.state, attr)\n        except AttributeError:\n            if attr in self.state._globals:\n                return self.state._globals[attr]\n            raise NameError(attr) from None\n\nclass State:\n    parent = None\n    _state = {}\n    _globals = {}\n    def __init__(self, **kwargs):\n        self._globals = sys.modules[self.__module__].__dict__\n        for argname, argvalue in kwargs.items():\n            super().__setattr__(argname, argvalue)\n        for argname, argvalue in kwargs.items():\n            if argname == \"parent\":\n                continue\n            setattr(self, argname, argvalue)    \n\n    def __setattr__(self, attr, value):\n        if attr == \"parent\" or attr.startswith(\"_\"):\n            super().__setattr__(attr, value)\n            return\n        if attr not in self._state:\n            raise AttributeError(attr)\n        if value is None:\n            super().__setattr__(attr, value)\n            return\n        type_descr = self._state[attr]\n        validator = None\n        validator_args = None\n        if isinstance(type_descr, str):\n            typename = type_descr\n        else:\n            assert isinstance(type_descr, tuple), (attr, type_descr)\n            typename = type_descr[0]\n            if len(type_descr) == 2:\n                if callable(type_descr[1]):\n                    validator = type_descr[1]\n                else:\n                    assert typename in default_validators, typename\n                    validator = default_validators[typename]\n                    validator_args = type_descr[1]\n            else:\n                assert len(type_descr) == 3\n                assert callable(type_descr[1])\n                validator, validator_args = type_descr[1:]\n        #typeclass = self._globals[typename]\n        typeclass = self._eval_in_scope(typename, True)\n        if isinstance(value, BASICTYPES):\n            if issubclass(typeclass, State):\n                if not isinstance(value, dict):\n                    raise TypeError(attr, typeclass, type(value))\n                value = value.copy()\n                value.pop(\"parent\", None)\n                value = typeclass(**value, parent=self)\n            else:\n                cast_value = typeclass(value)\n                if type(value)(cast_value) != value:\n                    raise TypeError(type(value), value, type(cast_value), cast_value)\n                value = cast_value\n        if not isinstance(value, typeclass):\n            raise TypeError(attr, type(attr), typeclass)\n        if validator is None:\n            if typename in default_validators:\n                validator = default_validators[typename]            \n        if validator is not None:\n            if validator_args is None:\n                validator(value, self)\n            else:\n                validator(value, self, validator_args)\n        super().__setattr__(attr, value)\n        if isinstance(value, State):\n            value.parent = self\n            value._validate()\n        self._validate()\n\n    def __getattr__(self, attr):\n        in_state = False\n        stateholder = self\n        while stateholder is not None:\n            if attr in stateholder._state:\n                in_state = True\n                break\n            stateholder = stateholder.parent\n        if not in_state:\n            raise AttributeError(attr)\n        parent = self.parent\n        if parent is None:\n            return None\n        try:\n            return getattr(parent, attr)\n        except AttributeError:\n            return None\n\n    def _eval_in_scope2(self, scope, expr, evaluate_strings, *, nest):\n        if isinstance(expr, str):\n            if nest > 0 and not evaluate_strings:\n                return expr\n            result = eval(expr, scope)\n            return result\n        elif isinstance(expr, dict):\n            result = {}\n            for k, v in expr.items():\n                eval_v = self._eval_in_scope2(\n                    scope, v, evaluate_strings, \n                    nest=nest+1\n                )\n                result[k] = eval_v\n            return result\n        elif isinstance(expr, (list, tuple)):\n            result = []\n            for v in expr.items():\n                eval_v = self._eval_in_scope2(\n                    scope, v, evaluate_strings, \n                    nest=nest+1\n                )\n                result.append(eval_v)\n            return tuple(result)\n        else:\n            return expr       \n\n    def _eval_in_scope(self, expr: str, evaluate_strings: bool):\n        scope = _StateScope(self)\n        return self._eval_in_scope2(scope, expr, evaluate_strings, nest=0)\n\n    def _validate(self) -> None:\n        pass\n\n    def copy(self):\n        \"\"\"Returns a shallow copy\"\"\"\n        d = {}\n        for attr in self._state:\n            try:\n                v = self.__dict__[attr]\n            except KeyError:\n                continue            \n            if v is not None:\n                d[attr] = v\n        d[\"parent\"] = self.parent\n        return type(self)(**d)\n        \n    def __dir__(self):\n        return list(self._state.keys()) ",
    "dependencies": [],
    "language": "python"
  },
  "__init__": {
    "code": "",
    "dependencies": [],
    "language": "python"
  },
  "__name__": "nefertiti",
  "functions.__init__": {
    "code": "",
    "dependencies": [],
    "language": "python"
  },
  "functions.fraglib": {
    "code": "\"\"\" Functions related to fragment libraries\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nimport numpy as np\n\ndef prepare_fraglib_backbone(fraglib: np.ndarray) -> np.ndarray:\n    \"\"\"Prepares a backbone fragment library\n    ( array of shape (nfrag, fraglen,bbsize, 3) )\n    - converts it to x,y,z,w form\n    - centers each fragment\n    Returns:\n    - Centered fragments, shape (nfrag, fraglen, bbsize, 4)\n    - Residuals (sum of squares) of each fragment\n    \"\"\"\n    assert fraglib.ndim == 4, fraglib.shape\n    assert fraglib.shape[-1] == 3, fraglib.shape\n    fraglib_com = fraglib.reshape(len(fraglib), -1, 3).mean(axis=1)\n    fraglib = fraglib - fraglib_com[:, None, None, :]\n    fraglib4 = np.ones(fraglib.shape[:-1] + (4,))\n    fraglib4[:, :, :, :3] = fraglib\n    residuals = np.einsum(\"ijkl,ijkl->i\", fraglib, fraglib)\n    return fraglib4, residuals\n\ndef calc_fraglib_matrices(\n    fraglib: np.ndarray,\n    pre_indices: np.ndarray,\n    post_indices: np.ndarray\n):\n    \"\"\"Calculate fragment-fragment superposition matrices\n    for a fragment library ( array of shape (nfrag, natoms, 3-or-4) )\n\n    Returns an array M of (nfrag x nfrag) 4x4 matrices,\n    (i.e. shape (nfrag, nfrag, 4, 4))\n    where each 4x4 matrix M[i,j] contains the superposition \n      of fragment i and fragment j,\n    where fragment i precedes fragment j in a trajectory of fragments\n\n    pre_indices is an array of integers to define \n    which atoms to select from fragment i\n\n    post_indices is the same for fragment j\n\n    For each matrix MM in M,\n    MM[:3, :3] is the rotation, MM[3, :3] is the translation,\n    and MM[3,3]=1\n    \n    The matrices can be applied to fragment coordinates using\n    nefertiti.functions.matrix.dotmat\n    \"\"\"\n    assert fraglib.ndim == 3, fraglib.shape\n    assert fraglib.shape[-1] in (3,4), fraglib.shape\n    assert pre_indices.ndim == 1\n    assert post_indices.ndim == 1\n\n    from .superimpose import superimpose_array\n\n    fraglib3 = fraglib[:, :, :3]\n    first = fraglib3[:, pre_indices]\n    first_com = first.mean(axis=1)\n    first = first - first_com[:, None, :]\n    matrices = np.zeros((len(fraglib),len(fraglib), 4, 4))\n    matrices[:, :, 3, 3] = 1\n    for n in range(len(fraglib)):\n        last = fraglib3[n, post_indices]\n        last_com = last.mean(axis=0)\n        last = last - last_com\n        curr_rotmats, _ = superimpose_array(first, last)\n        matrices[n, :, :3, :3] = curr_rotmats\n        matrices[n, :, 3, :3] = last_com[None] - first_com\n    return matrices    \n\ndef calc_fraglib_matrices_backbone(\n    fraglib: np.ndarray,\n):\n    \"\"\"Calculates fragment-fragment superposition matrices\n    for a backbone fragment library, \n    an array of shape (nfrag, nresidues, nbbatoms, 3-or-4)\n\n    See calc_fraglib_matrices for more details\n    \"\"\"\n    assert fraglib.ndim == 4, fraglib.shape\n    assert fraglib.shape[-1] in (3,4), fraglib.shape\n\n    natoms = fraglib.shape[1] * fraglib.shape[2]\n    pre_indices = np.arange(natoms - fraglib.shape[2])\n    post_indices = np.arange(fraglib.shape[2], natoms)\n    return calc_fraglib_matrices(\n        fraglib.reshape(len(fraglib), natoms, -1),\n        pre_indices,\n        post_indices\n    )",
    "dependencies": [],
    "language": "python"
  },
  "functions.matrix": {
    "code": "import numpy as np\n\ndef matmult(\n    curr_matrices: np.ndarray, \n    mult_matrices: np.ndarray, \n    indices: np.ndarray\n) -> np.ndarray:\n    \"\"\"Matrix multiplication\n    No verification of array shape is done!\n\n    Inputs: \n    - curr_matrices (cm), of shape Nx4x4\n    - mult_matrices (mm) of shape M1xM2x4x4\n    - Indices (ind) of shape Kx3, \n        with ind[:,0] in range 0..N\n        and ind[:, 1] range 0..M1\n        and ind[:, 2] range 0..M2\n    \n    returns r of shape Kx4x4\n    \n    for each k in range(K),\n      r[k] = mm[m1,m2].dot(cm[n]),\n    where n = ind[k][0] \n     and m1 = ind[k][1]\n     and m2 = ind[k][2]\n    \"\"\"\n    cm = curr_matrices[indices[:, 0]]\n    m1, m2 = indices[:, 1], indices[:, 2]\n    mm = mult_matrices[m1, m2]\n    result = np.einsum(\"ijk,ikl->ijl\", mm, cm) #diagonally broadcasted form of mm.dot(cm)\n    return result\n\n\ndef dotmat(\n    matrices: np.ndarray, \n    vectors: np.ndarray, \n    vector_indices: np.ndarray\n) -> np.ndarray:\n    \"\"\"Vector-matrix multiplication\n    No verification of array shape is done!\n\n    Inputs: \n    - vectors of shape NxVx4\n    - matrices of shape Mx4x4\n    - Indices (ind) of shape M, \n        with ind in range 0..N\n    \n    returns r of shape MxVx3\n    \n    for each m in range(M),\n      r[m] = vector[ind[m]].dot(matrix[m])\n    and then the 4-dimensional vector x,y,z,w has dimension w removed\n    \"\"\"\n    vv = vectors[vector_indices]\n    result = np.einsum(\"ijk,ikl->ijl\", vv, matrices) #diagonally broadcasted form of vv.dot(matrices)\n    return result[:, :, :3]",
    "dependencies": [],
    "language": "python"
  },
  "functions.parse_pdb": {
    "code": "\"\"\"Parse a PDB file into binary format\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nimport warnings\nimport Bio.PDB\nfrom Bio.PDB.StructureBuilder import PDBConstructionWarning\nwarnings.simplefilter('ignore', PDBConstructionWarning)\nfrom io import StringIO\nimport numpy as np\nfrom typing import List\n\natomic_dtype = [\n    (\"model\", 'uint16'),            \n    (\"hetero\", \"S1\"),\n    (\"name\", \"S4\"),\n    (\"altloc\",\"S1\"),\n    (\"resname\", \"S3\"),            \n    (\"chain\",\"S1\"),\n    (\"index\", 'uint32'),\n    (\"icode\", \"S1\"), \n    (\"resid\", 'uint16'),            \n    (\"x\", 'float32'),\n    (\"y\", 'float32'),\n    (\"z\", 'float32'),\n    (\"occupancy\", 'float32'),\n    (\"bfactor\", 'float32'),\n    (\"segid\", \"S4\"),\n    (\"element\", \"S2\")                  \n]\n\natomic_dtype = np.dtype(atomic_dtype, align=True)\n\ndef parse_pdb(pdbdata: str) -> np.ndarray:\n    \n    pdb_obj = StringIO(pdbdata)\n    \n    p = Bio.PDB.PDBParser()\n    struc = p.get_structure(\"PDB\", pdb_obj)\n    natoms = len(list(struc.get_atoms()))        \n    atomstate = np.zeros(natoms,dtype=atomic_dtype)\n    \n    a = atomstate\n    count = 0\n    for modelnr, model in enumerate(struc.get_models()):\n        atomlist = list(model.get_atoms())\n        atomlist.sort(key=lambda atom: atom.serial_number)\n        for atom in atomlist:\n            residue = atom.get_parent()\n            hetero, resid, icode = residue.get_id()\n            segid = residue.segid\n            resname = residue.resname\n            chainid = residue.get_parent().id\n            aa = a[count]\n            aa[\"model\"] = modelnr + 1\n            aa[\"hetero\"] = hetero\n            aa[\"name\"] = atom.name\n            aa[\"altloc\"] = atom.altloc\n            aa[\"resname\"] = resname\n            aa[\"chain\"] = chainid\n            aa[\"index\"] = atom.serial_number\n            aa[\"icode\"] = icode\n            aa[\"resid\"] = resid\n            aa[\"x\"] = atom.coord[0]\n            aa[\"y\"] = atom.coord[1]\n            aa[\"z\"] = atom.coord[2]\n            occ = atom.occupancy\n            if occ is None or occ < 0:\n                occ = 0\n            aa[\"occupancy\"] = occ\n            aa[\"segid\"] = segid\n            aa[\"element\"] = atom.element\n            count += 1\n    return atomstate\n\ndef get_xyz(struc: np.ndarray) -> np.ndarray:\n    struc2 = struc.flatten()\n    result = np.zeros((len(struc2), 3))\n    result[:, 0] = struc2[\"x\"]\n    result[:, 1] = struc2[\"y\"]\n    result[:, 2] = struc2[\"z\"]\n    return result.reshape(struc.shape + (3,))\n\ndef get_backbone(struc: np.ndarray, backbone_atoms: List[str]) -> np.ndarray:\n    assert struc.ndim == 1\n    selections = []\n    for atomnr, atom in enumerate(backbone_atoms):\n        mask = (struc[\"name\"] == atom.encode())\n        selection = struc[mask]\n        if atomnr > 0:\n            assert len(selection) == len(selections[0])\n        selections.append(selection)\n    result = np.empty((len(selections[0]), len(backbone_atoms)),atomic_dtype)\n    for snr, s in enumerate(selections):\n        result[:, snr] = s\n    return result\n\ndef get_sequence(struc:np.ndarray) -> str:\n    code = {\n        \"ALA\": \"A\",\n        \"CYS\": \"C\",\n        \"ASP\": \"D\",\n        \"GLU\": \"E\",\n        \"PHE\": \"F\",\n        \"GLY\": \"G\",\n        \"HIS\": \"H\",\n        \"ILE\": \"I\",\n        \"LYS\": \"K\",\n        \"LEU\": \"L\",\n        \"MET\": \"M\",\n        \"ASN\": \"N\",\n        \"PRO\": \"P\",\n        \"GLN\": \"Q\",\n        \"ARG\": \"R\",\n        \"SER\": \"S\",\n        \"THR\": \"T\",\n        \"VAL\": \"V\",\n        \"TRP\": \"W\",\n        \"TYR\": \"Y\",\n    }\n    result = \"\"\n    assert struc.ndim == 1\n    res = None\n    for a in struc:\n        if a[\"altloc\"].decode() not in (\"A\", \" \"):\n            continue\n        curr_res = a[\"model\"], a[\"chain\"], a[\"icode\"], a[\"resid\"]\n        if res == curr_res:\n            continue\n        res = curr_res\n        c = a[\"resname\"]\n        aa = code.get(c.decode(), \"X\")\n        result += aa\n    return result\n\nif __name__ == \"__main__\":\n    import sys\n    pdbfile = sys.argv[1]\n    outfile = sys.argv[2]\n    data = parse_pdb(open(pdbfile).read())\n    np.save(outfile, data, allow_pickle=False)",
    "dependencies": [],
    "language": "python"
  },
  "functions.prepare_backbone": {
    "code": "\"\"\" (description)\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nimport numpy as np\ndef prepare_backbone(struc, fraglen, bblen=4):\n    \"\"\"Prepares a backbone structure for RMSD calculation\n    Input: \n      struc: backbone structure of shape MxBx3 or B*Mx3, where B is bblen\n      M is the number of residues\n      fraglen: length (in residues) of the backbone fragment library\n      bblen: the number of backbone atoms (by default 4, i.e. N, CA, C, O)\n    Output:\n    - Structure of shape MxBx4, in (x,y,z,1) form\n    - Structure of shape KxFxBx4, where F is fraglen and B is bblen\n      The structure is in (x,y,z,1) form\n      K = M-F+1\n      each fragment k in K contains the coordinates for residue k:k+F\n      \n      This means that residues beyond the first (and before the last) get duplicated\n      The structure is centered so that the average atom (after duplication!) is (0,0,0)\n    \n    - Residuals (sum of squares) of the fragments\n      K residuals are returned.\n      Each residual is for that fragment, after centering it\n\n    - Center-of-masses of the fragments\n      This is the center-of-mass of each fragment within the centered structure\n    \"\"\"\n    refe = struc.reshape(-1, bblen, 3)\n    k = len(refe) - fraglen + 1\n    result = np.zeros((k, fraglen, bblen, 3))\n    residuals = np.zeros(k)\n    fragcoms = np.zeros((k, 3))\n    for kk in range(k):\n        result[kk] = refe[kk:kk+fraglen]\n    com = result.reshape(-1, 3).mean(axis=0)\n    result -= com\n    for kk in range(k):\n        frag = refe[kk:kk+fraglen].reshape(-1, 3) - com\n        fragcom = frag.mean(axis=0)\n        fragcoms[kk] = fragcom\n        fragc = frag - fragcom\n        residuals[kk] = (fragc*fragc).sum()\n    refe4 = np.ones(refe.shape[:-1]+(4,))\n    refe4[:, :, :3] = refe - com\n    refe_frag4 =  np.ones(result.shape[:-1]+(4,))\n    refe_frag4[:, :, :, :3] = result\n    return refe4, refe_frag4, residuals, fragcoms",
    "dependencies": [],
    "language": "python"
  },
  "functions.superimpose": {
    "code": "\"\"\"Functions related to Kabsch superimposition\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nimport numpy as np\nfrom numpy import sqrt\nfrom numpy.linalg import svd, det\n\ndef superimpose(coor1, coor2):\n    \"\"\"Returns the rotation matrix and RMSD between coor1 and coor2\n\n    Do coor1.dot(rotmat) to perform the superposition\n    \"\"\"\n    assert coor1.ndim == 2 and coor1.shape[-1] == 3, coor1.shape\n    assert coor2.ndim == 2 and coor2.shape[-1] == 3, coor2.shape\n    assert coor2.shape[0] == coor1.shape[0]\n    natoms = coor1.shape[0]\n\n    c1 = coor1 - coor1.mean(axis=0)\n    c2 = coor2 - coor2.mean(axis=0)\n    \n    residual1 = (c1*c1).sum()\n    residual2 = (c2*c2).sum()\n    covar = c1.T.dot(c2)   \n\n    v, s, wt = svd(covar)\n    if det(v) * det(wt) < 0:\n        s[-1] *= -1\n        v[:, -1] *= -1\n    rotmat = v.dot(wt)\n    ss = (residual1 + residual2) - 2 * s.sum()\n    if ss < 0:\n        ss = 0\n    rmsd = sqrt(ss / natoms)\n    return rotmat, rmsd\n\ndef superimpose_array(coor1_array, coor2):\n    \"\"\"Returns the rotation matrix and RMSD between coor1 and coor2\n    where coor1 is every element of coor1_array\n\n    Do np.einsum(\"ijk,ikl->ijl\", coor1_array, rotmat) to perform the superpositions\n    \"\"\"\n    assert coor1_array.ndim == 3 and coor1_array.shape[-1] == 3, coor1_array.shape\n    assert coor2.ndim == 2 and coor2.shape[-1] == 3, coor2.shape\n    assert coor2.shape[0] == coor1_array.shape[1]\n    natoms = coor2.shape[0]\n\n    c1_array = coor1_array - coor1_array.mean(axis=1)[:, None, :]\n    c2 = coor2 - coor2.mean(axis=0)\n    \n    residual1 = np.einsum(\"ijk,ijk->i\", c1_array, c1_array)\n    residual2 = (c2*c2).sum()\n    covar = np.einsum(\"ijk,jl->ikl\", c1_array, c2)\n\n    v, s, wt = svd(covar)\n    reflect = det(v) * det(wt)\n    s[:,-1] *= reflect\n    v[:, :, -1] *= reflect[:, None]\n    rotmat = np.einsum('...ij,...jk->...ik', v, wt)\n    ss = (residual1 + residual2) - 2 * s.sum(axis=1)\n    ss = np.maximum(ss, 0)\n    rmsd = sqrt(ss / natoms)\n    return rotmat, rmsd    \n\ndef superimpose_array_from_covar(covar, residuals, natoms, return_sd:bool):\n    \"\"\"Returns the rotation matrix and RMSD between coor1 and coor2\n    where coor1 is every element of coor1_array\n    \n    Instead of the coordinates themselves, provide the covariance\n     and residuals.\n\n    Do np.einsum(\"ijk,ikl->ijl\", coor1_array, rotmat) to perform the superpositions\n\n    If return_sd, calculate the SD instead of (rotmat, rmsd)\n    \"\"\"\n    v, s, wt = svd(covar)\n    reflect = det(v) * det(wt)\n    s[:,-1] *= reflect\n    sd = residuals - 2 * s.sum(axis=1)\n    sd = np.maximum(sd, 0)\n    if not return_sd:\n        v[:, :, -1] *= reflect[:, None]\n        rotmat = np.einsum('...ij,...jk->...ik', v, wt)\n        rmsd = sqrt(sd / natoms)\n        return rotmat, rmsd    \n    else:\n        return sd",
    "dependencies": [],
    "language": "python"
  },
  "functions.write_pdb": {
    "code": "\"\"\"Write binary format back to PDB file\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\nimport numpy as np\nfrom typing import List\n\ncode = {\n  \"ALA\": \"A\",\n  \"CYS\": \"C\",\n  \"ASP\": \"D\",\n  \"GLU\": \"E\",\n  \"PHE\": \"F\",\n  \"GLY\": \"G\",\n  \"HIS\": \"H\",\n  \"ILE\": \"I\",\n  \"LYS\": \"K\",\n  \"LEU\": \"L\",\n  \"MET\": \"M\",\n  \"ASN\": \"N\",\n  \"PRO\": \"P\",\n  \"GLN\": \"Q\",\n  \"ARG\": \"R\",\n  \"SER\": \"S\",\n  \"THR\": \"T\",\n  \"VAL\": \"V\",\n  \"TRP\": \"W\",\n  \"TYR\": \"Y\",\n  \"UNK\": \"X\",\n}\ncode_rev = {v:k for k,v in code.items()}\n\n# adapted from Bio.PDB.PDBIO.py\n\n_ATOM_FORMAT_STRING = (\n    \"%s%5i %-4s%c%3s %c%4i%c   %8.3f%8.3f%8.3f%6.2f%6.2f      %4s%2s%2s\\n\"\n)\ndef write_pdb_atom(atom) -> str:\n    name = atom[\"name\"].decode()\n    if not name.startswith(\"H\"):\n        name = \" \" + name\n    occ = atom[\"occupancy\"]\n    if occ > 100:\n        occ = 100\n    if occ < 0:\n        occ = 0\n    args = (\n        \"ATOM  \" if atom[\"hetero\"].decode().strip() == \"\" else \"HETATM\",\n        atom[\"index\"],\n        name,\n        atom[\"altloc\"].decode(),\n        atom[\"resname\"].decode(),\n        atom[\"chain\"].decode(),\n        atom[\"resid\"],\n        atom[\"icode\"].decode(),\n        atom[\"x\"],\n        atom[\"y\"],\n        atom[\"z\"],\n        occ,\n        np.minimum(atom[\"bfactor\"], 100),\n        atom[\"segid\"].decode(),\n        atom[\"element\"].decode(),\n        \"\",\n    )\n    return _ATOM_FORMAT_STRING % args\n#/adapted\n\ndef write_pdb(struc: np.ndarray) -> str:\n    from . import parse_pdb\n    assert struc.dtype == parse_pdb.atomic_dtype\n    assert struc.ndim == 1\n    pdb = \"\"\n    for atom in struc:\n        line = write_pdb_atom(atom)\n        pdb += line\n    return pdb\n\ndef build_pdb_backbone(\n    struc: np.ndarray, \n    bb_atoms: List[str],\n    sequence:str = None,\n    *,\n    atomindex_offset=0,\n    resid_offset=0,\n) -> np.ndarray:\n    assert struc.ndim == 3, struc.shape\n    assert struc.shape[1] == len(bb_atoms), struc.shape\n    assert struc.shape[2] in (3,4), struc.shape\n    assert len(struc) < 10000\n    if sequence is not None:\n        assert len(sequence) == len(struc)\n    from . import parse_pdb\n    newstruc = np.zeros(\n        (len(struc), len(bb_atoms)),\n        parse_pdb.atomic_dtype\n    )\n    for resnr in range(len(struc)):\n        res = newstruc[resnr]\n        for anr in range(len(bb_atoms)):\n            s = struc[resnr][anr]\n            a = newstruc[resnr][anr]\n            a[\"model\"] = 1\n            a[\"hetero\"] = \"\"\n            a[\"name\"] = bb_atoms[anr].encode()\n            a[\"altloc\"] = \" \"\n            resname = code_rev[sequence[resnr]] if sequence is not None else \"ALA\"\n            a[\"resname\"] = resname.encode()\n            a[\"chain\"] = \" \"\n            a[\"index\"] = len(bb_atoms) * resnr + anr + atomindex_offset + 1\n            a[\"icode\"] = \" \"\n            a[\"resid\"] = resnr + resid_offset + 1\n            a[\"x\"] = s[0]\n            a[\"y\"] = s[1]\n            a[\"z\"] = s[2]\n            a[\"occupancy\"] = 1\n            a[\"segid\"] = \"\"\n            a[\"element\"] = bb_atoms[anr][0].encode()\n    return newstruc.flatten()\n\ndef build_pdb_fragment_backbone(\n    struc: np.ndarray, \n    bb_atoms: List[str],\n    sequence:str = None\n):\n    assert struc.ndim == 4\n    assert struc.shape[2] == len(bb_atoms)\n    assert struc.shape[3] in (3,4)\n    struc = struc[:, :, :, :3]\n    nfrag, fraglen = struc.shape[:2]\n    if sequence is not None:\n        assert len(sequence) == nfrag + fraglen - 1, (len(sequence), nfrag, fraglen)\n    assert fraglen < 100\n    offset = 10 if fraglen < 10 else 100\n    assert len(sequence) * offset < 10000\n\n    from . import parse_pdb\n    atoms = np.empty(nfrag * fraglen * len(bb_atoms), parse_pdb.atomic_dtype)\n    for n in range(nfrag):\n        stride = fraglen * len(bb_atoms)\n        atoms[n*stride:(n+1)*stride] = build_pdb_backbone(\n            struc[n],\n            bb_atoms,\n            sequence[n:n+fraglen],\n            atomindex_offset=n * len(bb_atoms) * fraglen,\n            resid_offset= n * offset\n        )\n    return atoms\n    ",
    "dependencies": [],
    "language": "python"
  },
  "procedures.__init__": {
    "code": "",
    "dependencies": [],
    "language": "python"
  },
  "procedures.greedy": {
    "code": "\"\"\" Builds fragment trajectories in a greedy way\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nimport numpy as np\n\nfrom nefertiti.MainState import (\n    MainState, Stage, FragmentLibrary, StructureRepresentation\n)\nfrom nefertiti.progressions.prepare_backbone import (\n    prepare_backbone, prepare_backbone_from_pdb\n)\nfrom nefertiti.progressions.fraglib import load_backbone_fraglib\nfrom nefertiti.progressions.greedy import greedy as _greedy\nfrom nefertiti.progressions.grow import init, init_coor_backbone\nfrom nefertiti.progressions.coor import update_coordinates_backbone\nfrom nefertiti.progressions.rmsd import (\n    update_residuals_covar_backbone,\n    rmsd_backbone,\n)\n\ndef greedy_backbone_rmsd(\n    refe, #str | np.ndarray,\n    fraglib: np.ndarray,\n    *,\n    format: str,\n    poolsize: int,\n    chunksize: int=100000,\n    bb_atoms = [\"N\", \"CA\", \"C\", \"O\"]\n) -> MainState:\n    \"\"\"\n    Builds near-native fragment trajectories in a greedy way\n    using a fragment-duplicated backbone RMSD\n\n    Input: \n    - a reference structure with nres residues.\n      \"format\" indicates the format.\n      - \"pdb\": in PDB format (text string)\n      - \"npy\": as a shape=(nresidues, len(bb_atoms), 3) numpy array\n    - a fragment library of shape (nfrag, fraglen, len(bb_atoms), 3)\n\n    poolsize: size of the greedy pool \n    (keep the best <poolsize> RMSDs at each trajectory growing stage)\n\n    chunksize: maximum storage space per stage. Affects memory requirements:\n    What is stored at each stage N: \n    - an array of shape (chunksize, N, 3) fragment centers-of-mass\n    - an array of shape (chunksize, N) trajectories (ints)\n    - an array of shape (chunksize, 4, 4) matrices\n    - an array of shape (chunksize, 3, 3) covariance matrices\n    - an array of shape (chunksize,) residuals\n    - an array of shape (chunksize, len(bb_atoms), 3) coordinates\n    - an array of shape (chunksize, 3) scores\n    Normally, N goes from 1 to nres-fraglen+1, but buffers are swapped,\n     so there are only 2 independent stages.\n\n    bb_atoms: a list of backbone atoms. default: [\"N\", \"CA\", \"C\", \"O\"]\n\n    Returns:\n    The MainState object m. The interesting results are mostly in m.stages[-1]\n    \"\"\"\n    if chunksize < poolsize:\n        raise ValueError(\"chunksize must be at least poolsize\")\n\n    fraglen = fraglib.shape[1]\n\n    assert fraglib.shape[2] == len(bb_atoms), (fraglib.shape, bb_atoms)\n    \n    s = StructureRepresentation()\n    s.bb_atoms = bb_atoms\n    if format == \"pdb\":\n        prepare_backbone_from_pdb(s, refe)\n    elif format == \"npy\":\n        prepare_backbone(s, refe)\n    else:\n        raise ValueError(format)\n    \n    nres = s.nresidues\n    if fraglen > nres:\n        raise ValueError(\"Cannot grow reference structures shorter than the size of a fragment\")\n    nstages = nres - fraglen + 1\n\n    ms = MainState()\n    ms.fraglen = fraglen\n    ms.refe = s\n    ms.nfrags = nstages\n    ms.bb_atoms = bb_atoms\n    f = FragmentLibrary()\n    f.nfrags = len(fraglib)  # Here: fragment library size\n    ms.fraglib = f\n    load_backbone_fraglib(f, fraglib)\n    init(ms, nstages=nstages, maxsize=chunksize, \n        with_coor=True,\n        with_rmsd=True,\n        with_all_matrices=False,\n        swap_buffers=True\n    )\n    init_coor_backbone(ms, fraglen, len(bb_atoms))\n    \n    updaters = [\n        update_coordinates_backbone,\n        update_residuals_covar_backbone\n    ]\n    _greedy(\n        ms,\n        scorer=rmsd_backbone,\n        updaters=updaters,\n        poolsize=poolsize\n    )\n    \n    return ms",
    "dependencies": [
      ".MainState",
      ".progressions.coor",
      ".progressions.fraglib",
      ".progressions.greedy",
      ".progressions.grow",
      ".progressions.prepare_backbone",
      ".progressions.rmsd"
    ],
    "language": "python"
  },
  "procedures.kbest": {
    "code": "\"\"\"Enumerates the k-best fragment trajectories\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nfrom nefertiti.procedures.greedy import greedy_backbone_rmsd\nimport numpy as np\n\nfrom nefertiti.MainState import (\n    MainState, Stage, FragmentLibrary, StructureRepresentation\n)\nfrom nefertiti.progressions.prepare_backbone import (\n    prepare_backbone, prepare_backbone_from_pdb,\n    select_last_backbone\n)\nfrom nefertiti.progressions.fraglib import load_backbone_fraglib\nfrom nefertiti.progressions.kbest import kbest as _kbest\nfrom nefertiti.progressions.greedy import greedy as _greedy\nfrom nefertiti.progressions.grow import init, init_coor_backbone\nfrom nefertiti.progressions.coor import update_coordinates_backbone\nfrom nefertiti.progressions.rmsd import (\n    update_residuals_covar_backbone,\n    rmsd_backbone,\n)\n\ndef kbest_backbone_rmsd(\n    refe, #str | np.ndarray,\n    fraglib: np.ndarray,\n    *,\n    format: str,\n    k: int,\n    maxblocksize: int=50000,\n    minblocksize: int=10000,\n    bb_atoms = [\"N\", \"CA\", \"C\", \"O\"]\n) -> MainState:\n    \"\"\"\n    Enumerates the k-best near-native fragment trajectories\n    using a fragment-duplicated backbone RMSD\n    Input: \n    - a reference structure with nres residues.\n      \"format\" indicates the format.\n      - \"pdb\": in PDB format (text string)\n      - \"npy\": as a shape=(nresidues, len(bb_atoms), 3) numpy array\n    - a fragment library of shape (nfrag, fraglen, len(bb_atoms), 3)\n\n    k: the best trajectories to keep\n\n    minblocksize: minimum block size to send to the next stage.\n    Keep this small to reach an initial estimate soon (leading to efficient pruning)\n    Don't make it too small (minblocksize * nfrag should be in the thousands)\n    to keep Python function call overhead reasonable.\n\n    maxblocksize: maximum storage space per stage. Affects memory requirements:\n    What is stored at each stage N: \n    - an array of shape (chunksize, N, 3) fragment centers-of-mass\n    - an array of shape (chunksize, N) trajectories (ints)\n    - an array of shape (chunksize, 4, 4) matrices\n    - an array of shape (chunksize, 3, 3) covariance matrices\n    - an array of shape (chunksize,) residuals\n    - an array of shape (chunksize, len(bb_atoms), 3) coordinates\n    - an array of shape (chunksize, 3) scores\n    Normally, N goes from 1 to nres-fraglen+1, but buffers are swapped,\n     so there are only 2 independent stages.\n\n    bb_atoms: a list of backbone atoms. default: [\"N\", \"CA\", \"C\", \"O\"]\n\n    Returns:\n    The MainState object m. The interesting results are mostly in m.stages[-1]\n    \"\"\"\n    assert minblocksize < maxblocksize\n\n    fraglen = fraglib.shape[1]\n\n    assert fraglib.shape[2] == len(bb_atoms), (fraglib.shape, bb_atoms)\n    \n    s = StructureRepresentation()\n    s.bb_atoms = bb_atoms\n    if format == \"pdb\":\n        prepare_backbone_from_pdb(s, refe)\n    elif format == \"npy\":\n        prepare_backbone(s, refe)\n    else:\n        raise ValueError(format)\n    s_copy = s.copy()\n\n    nres = s.nresidues\n    if fraglen > nres:\n        raise ValueError(\"Cannot grow reference structures shorter than the size of a fragment\")\n    nstages = nres - fraglen + 1\n\n    ms0 = MainState()\n    ms0.fraglen = fraglen\n    f = FragmentLibrary()\n    f.nfrags = len(fraglib)  # Here: fragment library size\n    ms0.fraglib = f\n    load_backbone_fraglib(f, fraglib)\n    ms0.bb_atoms = bb_atoms\n    ms_copy = ms0.copy()\n\n    updaters = [\n        update_coordinates_backbone,\n        update_residuals_covar_backbone\n    ]\n    def copy_s():\n        s = s_copy.copy()\n        s.coor_residue = s_copy.coor_residue.copy()\n        s.coor_fragment = s_copy.coor_fragment.copy()\n        return s\n\n    ms = None\n    def _greedy_iter(stagelen):\n        nonlocal ms\n        ms = ms_copy.copy()\n        s = copy_s()\n\n        ms.refe = s\n        ms.nfrags = stagelen\n        select_last_backbone(s, stagelen)\n\n        init(ms, nstages=stagelen, \n            maxsize=maxblocksize, \n            with_coor=True,\n            with_rmsd=True,\n            with_all_matrices=False,\n            swap_buffers=True\n        )\n        init_coor_backbone(ms, fraglen, len(bb_atoms))\n        \n        _greedy(\n            ms,\n            scorer=rmsd_backbone,\n            updaters=updaters,\n            poolsize=200\n        )\n\n    def _best_iter(stagelen, greedy_best, best_msds):\n        assert len(best_msds) == stagelen - 1\n        nonlocal ms\n        ms = ms_copy.copy()\n        s = copy_s()\n\n        ms.refe = s\n        select_last_backbone(s, stagelen)\n        ms.nfrags = stagelen\n\n        init(ms, nstages=stagelen, \n            maxsize=maxblocksize, \n            with_coor=True,\n            with_rmsd=True,\n            with_all_matrices=False,\n            swap_buffers=False\n        )\n        init_coor_backbone(ms, fraglen, len(bb_atoms))\n        \n        upper_estimate=None\n        if greedy_best is not None:\n            upper_estimate=greedy_best+0.001 #in case of rounding errors\n        _kbest(\n            ms,\n            scorer=rmsd_backbone,\n            updaters=updaters,\n            k=1,\n            minblocksize=minblocksize,\n            upper_estimate=upper_estimate,\n            downstream_best=best_msds,\n        )\n\n    best_msds = []\n    tot_nfrags = s_copy.nfrags\n    for n in range(tot_nfrags-1):\n        print(\"PRECALC {}/{}\".format(n+1, tot_nfrags-1))\n        _greedy_iter(n+1)\n        natoms = ms.nfrags * ms.fraglen * len(ms.bb_atoms)\n        greedy_best = ms.stages[-1].scores[0]        \n        print(\"GBEST\", n+1, greedy_best, np.sqrt(greedy_best/natoms))\n        _best_iter(n+1, greedy_best, best_msds)\n        best = ms.stages[-1].scores[0]\n        print(\"BEST \", n+1, best, np.sqrt(best/natoms))\n        best_msds.insert(0, best)\n        print()\n\n    upper_estimate = None\n    if k == 1:\n        _greedy_iter(tot_nfrags)\n        natoms = ms.nfrags * ms.fraglen * len(ms.bb_atoms)\n        greedy_best = ms.stages[-1].scores[0]\n        print(\"GBEST\", tot_nfrags, greedy_best, np.sqrt(greedy_best/natoms))\n        upper_estimate=greedy_best+0.001 #in case of rounding errors\n\n    ms.refe = s_copy.copy()\n    ms.nfrags = tot_nfrags\n\n    maxblocksizes = [maxblocksize] * tot_nfrags\n    maxblocksizes[-1] += k\n    init(ms, nstages=tot_nfrags, \n        maxsize=maxblocksizes, \n        with_coor=True,\n        with_rmsd=True,\n        with_all_matrices=False,\n        swap_buffers=False\n    )\n    init_coor_backbone(ms, fraglen, len(bb_atoms))\n\n    _kbest(\n        ms,\n        scorer=rmsd_backbone,\n        updaters=updaters,\n        k=k,\n        minblocksize=minblocksize,\n        upper_estimate=upper_estimate,\n        downstream_best=best_msds,\n    )\n\n    return ms",
    "dependencies": [
      ".MainState",
      ".procedures.greedy",
      ".progressions.coor",
      ".progressions.fraglib",
      ".progressions.greedy",
      ".progressions.grow",
      ".progressions.kbest",
      ".progressions.prepare_backbone",
      ".progressions.rmsd"
    ],
    "language": "python"
  },
  "procedures.randombest": {
    "code": "\"\"\" Randomly generate fragment trajectories\nwithin an (optional) score threshold\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nimport numpy as np\n\nfrom nefertiti.MainState import (\n    MainState, Stage, FragmentLibrary, StructureRepresentation\n)\nfrom nefertiti.progressions.prepare_backbone import (\n    prepare_backbone, prepare_backbone_from_pdb\n)\nfrom nefertiti.progressions.fraglib import load_backbone_fraglib\nfrom nefertiti.progressions.randombest import randombest as _randombest\nfrom nefertiti.progressions.grow import init, init_coor_backbone\nfrom nefertiti.progressions.coor import update_coordinates_backbone\nfrom nefertiti.progressions.rmsd import (\n    update_residuals_covar_backbone,\n    rmsd_backbone,\n)\n\ndef randombest_backbone_rmsd(\n    refe, #str | np.ndarray,\n    fraglib: np.ndarray,\n    *,\n    format: str,\n    ntrajectories: int,\n    max_rmsd: float,\n    use_downstream_best: bool,\n    minblocksize: int=2000,\n    bb_atoms = [\"N\", \"CA\", \"C\", \"O\"],\n    random_seed = 0,\n) -> MainState:\n    \"\"\"\n    Builds near-native fragment trajectories by random sampling\n    using a fragment-duplicated backbone RMSD\n\n    Input: \n    - a reference structure with nres residues.\n      \"format\" indicates the format.\n      - \"pdb\": in PDB format (text string)\n      - \"npy\": as a shape=(nresidues, len(bb_atoms), 3) numpy array\n    - a fragment library of shape (nfrag, fraglen, len(bb_atoms), 3)\n\n    ntrajectories: the number of trajectories to generate.\n\n    max_rmsd: maximum RMSD threshold for the generated trajectories\n\n    use_downstream_best: if true, first obtain the downstream best RMSDs\n     for fragment 2..N until the end. This will be very time-consuming for\n    large structures. However, this will then prune the search very \n    efficiently. Recommended if both the structure and the max_rmsd are\n    small.\n\n    minblocksize: maximum storage space per stage. Affects memory requirements.\n    Each stage has a size \"maxsize\", which is 2 x minblocksize, except for the\n    last stage, where it is ntrajectories + minblocksize.\n    \n    What is stored at each stage N: \n    - an array of shape (maxsize, N, 3) fragment centers-of-mass\n    - an array of shape (maxsize, N) trajectories (ints)\n    - an array of shape (maxsize, 4, 4) matrices\n    - an array of shape (maxsize, 3, 3) covariance matrices\n    - an array of shape (maxsize,) residuals\n    - an array of shape (maxsize, len(bb_atoms), 3) coordinates\n    - an array of shape (maxsize, 3) scores\n\n    bb_atoms: a list of backbone atoms. default: [\"N\", \"CA\", \"C\", \"O\"]\n\n    Returns:\n    The MainState object m. The interesting results are mostly in m.stages[-1]\n    \"\"\"\n    np.random.seed(random_seed)\n\n    from nefertiti.procedures.kbest import kbest_backbone_rmsd\n    fraglen = fraglib.shape[1]\n\n    assert fraglib.shape[2] == len(bb_atoms), (fraglib.shape, bb_atoms)\n    \n    s = StructureRepresentation()\n    s.bb_atoms = bb_atoms\n    if format == \"pdb\":\n        prepare_backbone_from_pdb(s, refe)\n    elif format == \"npy\":\n        prepare_backbone(s, refe)\n    else:\n        raise ValueError(format)\n    \n    nres = s.nresidues\n    if fraglen > nres:\n        raise ValueError(\"Cannot grow reference structures shorter than the size of a fragment\")\n\n    downstream_best = None    \n    if use_downstream_best and fraglen > 1:\n        refe2 = s.coor_residue.backbone[1:]\n        ms0 = kbest_backbone_rmsd(\n            refe2,\n            fraglib, \n            format=\"npy\",\n            k=1\n        )\n        downstream_best = ms0.downstream_best_score\n\n    nstages = nres - fraglen + 1\n\n    ms = MainState()\n    ms.fraglen = fraglen\n    ms.refe = s\n    ms.nfrags = nstages\n    ms.bb_atoms = bb_atoms\n    f = FragmentLibrary()\n    f.nfrags = len(fraglib)  # Here: fragment library size\n    ms.fraglib = f\n    load_backbone_fraglib(f, fraglib)\n    maxsizes = [2*minblocksize] * nstages\n    maxsizes[-1] = minblocksize + ntrajectories\n    init(ms, nstages=nstages, maxsize=maxsizes, \n        with_coor=True,\n        with_rmsd=True,\n        with_all_matrices=False,\n        swap_buffers=False\n    )\n    init_coor_backbone(ms, fraglen, len(bb_atoms))\n    \n    updaters = [\n        update_coordinates_backbone,\n        update_residuals_covar_backbone\n    ]\n    \n    natoms = nstages * fraglen * len(bb_atoms)\n    threshold = None\n    if max_rmsd is not None:\n        threshold = max_rmsd**2 * natoms\n    _randombest(\n        ms,\n        scorer=rmsd_backbone,\n        updaters=updaters,\n        ntrajectories=ntrajectories,\n        threshold=threshold,\n        minblocksize=minblocksize,\n        downstream_best=downstream_best\n    )\n    \n    return ms",
    "dependencies": [
      ".MainState",
      ".progressions.coor",
      ".progressions.fraglib",
      ".progressions.grow",
      ".progressions.prepare_backbone",
      ".progressions.randombest",
      ".progressions.rmsd"
    ],
    "language": "python"
  },
  "progressions.__init__": {
    "code": "",
    "dependencies": [],
    "language": "python"
  },
  "progressions.coor": {
    "code": "\"\"\"Progressions related to coordinate calculation\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\nfrom ..functions.matrix import dotmat\n\n\nfrom ..MainState import Stage\ndef update_coordinates_backbone(\n    s: Stage, newsize\n) -> None:\n    size = s.size\n    fraglib = s.fraglib\n    frags = fraglib.coor.backbone4_centered\n    fraglen, bbsize = frags.shape[1:3]\n    frags = frags.reshape(len(frags), -1, 4)\n    coor = s.coor.backbone\n    assert coor is not None  #.backbone\n    mats = s.matrices[size:newsize]\n    traj = s.trajectories[size:newsize, -1]\n    newcoor = dotmat(mats, frags, traj)[:, :, :3]\n    newcoor = newcoor.reshape(len(newcoor), fraglen, bbsize, 3)\n    coor[size:newsize] = newcoor",
    "dependencies": [
      ".MainState",
      ".functions.matrix"
    ],
    "language": "python"
  },
  "progressions.fraglib": {
    "code": "\"\"\" (description)\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\nimport numpy as np\n\nfrom ..MainState import FragmentCoordinateRepresentation, FragmentLibrary\n\ndef load_backbone_fraglib(\n    f: FragmentLibrary, \n    fraglib: np.ndarray\n) -> None:\n    \"\"\"Updates `f` by calling the functions \n    `prepare_fraglib_backbone` and  `calc_fraglib_matrices_backbone`\n    and setting:\n    - f.matrices\n    - f.coor.backbone4_centered\n    - f.coor.backbone_residuals\n\n    f.nfrags, f.fraglen and f.bb_atoms are set as well, if not defined.\n    \"\"\"\n    from ..functions.fraglib import (\n        prepare_fraglib_backbone, calc_fraglib_matrices_backbone\n    )\n    if f.nfrags is not None:\n        assert f.nfrags == fraglib.shape[0], (f.nfrags, fraglib.shape)\n    else:\n        f.nfrags = fraglib.shape[0]\n    \n    if f.fraglen is not None:\n        assert f.fraglen == fraglib.shape[1], (f.fraglen, fraglib.shape)\n    else:\n        f.fraglen = fraglib.shape[1]\n    \n    if f.bb_atoms is None:\n        assert fraglib.shape[2] == 4, fraglib.shape\n        f.bb_atoms = [\"N\", \"CA\", \"C\", \"O\"]\n    else:\n        assert len(f.bb_atoms) == fraglib.shape[2], (f.bb_atoms, fraglib.shape)\n\n    fraglib, residuals = prepare_fraglib_backbone(fraglib)\n    matrices = calc_fraglib_matrices_backbone(fraglib)\n    f.matrices = matrices\n    if f.coor is None:\n        f.coor = FragmentCoordinateRepresentation()\n    f.coor.backbone4_centered = fraglib\n    f.coor.backbone_residuals = residuals",
    "dependencies": [
      ".MainState"
    ],
    "language": "python"
  },
  "progressions.greedy": {
    "code": "\"\"\"Progressions to build fragment trajectories in a greedy way\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nfrom ..MainState import MainState\nfrom .grow import grow, sort_score\nimport time\nimport logging\nlogger = logging.getLogger(\"nefertiti\")\n\ndef greedy(\n    s: MainState,\n    scorer: callable,\n    updaters:list,\n    poolsize,\n) -> None:\n    s1 = s.stages[0]\n    grow(None, s1, scorer=scorer, updaters=updaters)\n    sort_score(s1)\n    if s1.size > poolsize:\n        s1.size = poolsize\n\n    active = 0\n    it = 0\n    t = time.time()\n    while active < len(s.stages) - 1:\n        it += 1\n\n        s1 = s.stages[active]\n        s2 = s.stages[active+1]\n        if s1.size == 0:\n            logger.info(\n                \"Greedy done: %d/%d, %.3f sec\", \n                active+1, len(s.stages), time.time()-t,\n                \n            )\n            active += 1\n            continue\n\n        #print(\"GREEDY GROW\", active+1)\n        grow(s1, s2, scorer=scorer, updaters=updaters)            \n        sort_score(s2)\n        if s2.size > poolsize:\n            s2.size = poolsize\n\n        \"\"\"\n        # Works only for backbone RMSD growing...    \n        if s2.size:\n            natoms = (active + 2) * s.fraglib.fraglen * len(s.bb_atoms) \n            import numpy as np \n            best_rmsd = np.sqrt(s2.scores[0] / natoms)\n            print(active+2, best_rmsd)\n            print()\n        # /\n        \"\"\"",
    "dependencies": [
      ".MainState",
      ".progressions.grow"
    ],
    "language": "python"
  },
  "progressions.grow": {
    "code": "\"\"\"Progressions to grow fragment trajectories by one fragment,\nadding them to the next stage\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\nimport numpy as np\nfrom typing import List\nfrom ..MainState import MainState, Stage\nfrom ..functions.matrix import matmult\n\n# From https://stackoverflow.com/a/11146645\ndef cartesian_product(arrays):\n    la = len(arrays)\n    dtype = np.result_type(*arrays)\n    arr = np.empty([la] + [len(a) for a in arrays], dtype=dtype)\n    for i, a in enumerate(np.ix_(*arrays)):\n        arr[i, ...] = a\n    return arr.reshape(la, -1).T\n    \ndef init(\n    ms: MainState, \n    nstages: int,\n    maxsize, #int | List[int],\n    *,\n    with_coor: bool,\n    with_rmsd: bool,\n    with_all_matrices: bool,\n    swap_buffers:bool,\n) -> None:\n    ms.stages = []\n    if isinstance(maxsize, int):\n        maxsize = [maxsize] * nstages\n    else:\n        assert len(maxsize) == nstages\n    \n    if swap_buffers:\n        allmx = max(maxsize)\n        trajs = [\n            np.zeros((allmx, nstages),np.uint16),\n            np.zeros((allmx, nstages),np.uint16),\n        ]        \n        if with_rmsd:\n            all_fragcoms = [\n                np.zeros((allmx, nstages, 3)),\n                np.zeros((allmx, nstages, 3)),\n            ]            \n        if with_all_matrices:\n            all_mats = [\n                np.zeros((allmx, nstages, 4, 4)),\n                np.zeros((allmx, nstages, 4, 4)),\n            ]\n        \n\n    for n in range(nstages):\n        stage = Stage()\n        stage.parent = ms\n\n        stage.size = 0\n        mx = maxsize[n]\n        stage.maxsize = mx\n        stage.fragindex = n+1\n\n        if with_coor:\n            stage.nfrags = mx\n            if swap_buffers and n > 1:\n                stage.coor = ms.stages[-2].coor\n            else:\n                stage.coor = {}\n\n        if mx is None:\n            continue\n\n        if swap_buffers:\n            traj = trajs[n%2]\n            stage.trajectories = traj[:mx, :n+1]\n        else:\n            stage.trajectories = np.zeros((mx, n+1),np.uint16)\n\n        if swap_buffers and n > 1:\n            stage.matrices = ms.stages[-2].matrices\n            stage.scores = ms.stages[-2].scores\n        else:\n            stage.matrices = np.zeros((mx, 4, 4))\n            stage.scores = np.zeros(mx)\n\n        if with_rmsd:\n            if swap_buffers:\n                fragcoms = all_fragcoms[n%2]\n                stage.fragcoms = fragcoms[:mx, :n+1]\n            else:\n                stage.fragcoms = np.zeros((mx, n+1, 3))\n\n            if swap_buffers and n > 1:\n                stage.covar = ms.stages[-2].covar\n                stage.residuals = ms.stages[-2].residuals\n            else:\n                stage.covar = np.zeros((mx, 3, 3))\n                stage.residuals = np.zeros(mx)\n        \n        if with_all_matrices:\n            if swap_buffers:\n                all_mat = all_mats[n%2]\n                stage.all_matrices = all_mat[:mx, :n+1]\n            else:\n                stage.all_matrices = np.zeros((mx, n+1, 4, 4))\n\n        ms.stages.append(stage)    \n\ndef _grow(stage1: Stage, stage2: Stage, *, scorer, updaters, random):\n    space = stage2.maxsize - stage2.size\n    fraglib = stage2.fraglib\n    transition_matrices = fraglib.matrices\n    fraglibsize = fraglib.nfrags\n    if not random:\n        assert space >= fraglibsize\n        ngrow = space // fraglibsize\n    else:\n        ngrow = space\n            \n    if not random:\n        if stage1 is None:\n            ngrow = 1\n            newtraj = np.arange(fraglibsize)[:, None]\n            newmatrices = np.eye(4)\n            ngrow2 = fraglibsize\n        else:  \n            ngrow = min(ngrow, stage1.size)\n            traj = stage1.trajectories[:ngrow]\n            traj_indices0 = cartesian_product((\n                np.arange(ngrow),            \n                np.arange(fraglibsize)\n            ))\n            traj_indices = np.empty((ngrow*fraglibsize, 3), int)\n            ind = traj_indices0[:, 0]\n            traj_indices[:, 0] = ind\n            traj_indices[:, 1] = traj[:, -1][ind]\n            traj_indices[:, 2] = traj_indices0[:, 1]\n    else:\n        ngrow2 = ngrow\n        if stage1 is None:\n            newtraj = np.random.choice(fraglibsize, ngrow)[:, None]\n            newmatrices = np.eye(4)\n        else:  \n            ngrow = min(ngrow, stage1.size)\n            traj = stage1.trajectories[:ngrow]\n            ind = np.arange(ngrow)\n            traj_indices = np.empty((ngrow, 3), int)\n            traj_indices[:, 0] = np.arange(len(traj))\n            traj_indices[:, 1] = traj[:, -1]\n            traj_indices[:, 2] = np.random.choice(fraglibsize, ngrow)\n\n    if stage1 is not None:\n        ngrow2 = len(traj_indices)\n        mat = stage1.matrices[:ngrow]\n        newmatrices = matmult(\n            mat, transition_matrices, traj_indices\n        )\n        newtraj0 = traj[ind]\n        newtraj = np.concatenate((newtraj0, traj_indices[:, 2,][:, None]), axis=1)\n        \n    newsize = stage2.size + ngrow2\n    stage2.trajectories[stage2.size: newsize] = newtraj\n    stage2.matrices[stage2.size:newsize] = newmatrices\n\n    if stage1 is not None:\n        if stage2.covar is not None:\n            stage2.covar[stage2.size:newsize] = stage1.covar[ind]\n        if stage2.residuals is not None:\n            stage2.residuals[stage2.size:newsize] = stage1.residuals[ind]\n        if stage2.fragcoms is not None:\n            if stage1 is not None:\n                stage2.fragcoms[stage2.size:newsize, :-1] = stage1.fragcoms[ind]\n    else:\n        if stage2.covar is not None:\n            stage2.covar[stage2.size:newsize] = 0\n        if stage2.residuals is not None:\n            stage2.residuals[stage2.size:newsize] = 0\n        if stage2.fragcoms is not None:\n            stage2.fragcoms[stage2.size:newsize] = 0\n\n    if stage2.all_matrices is not None:\n        if stage1 is not None:\n            stage2.all_matrices[stage2.size:newsize, :-1] = stage1.all_matrices[ind]\n        stage2.all_matrices[stage2.size:newsize, -1] = newmatrices\n\n    for updater in updaters:\n        updater(stage2, newsize)\n    scorer(stage2, newsize)\n\n    \"\"\"\n    # Works only for backbone RMSD growing...\n    natoms = stage2.fragindex * len(stage2.bb_atoms) * stage2.fraglen \n    best, worst = stage2.scores[:newsize].min(), stage2.scores[:newsize].max()  \n    print(\"!RMSD\", np.sqrt(best/natoms), np.sqrt(worst/natoms))\n    \"\"\"\n\n    stage2.size = newsize\n\n    if stage1 is not None: \n        oldsize = stage1.size\n        stage1.size -= ngrow\n        size = stage1.size\n        if size > 0:\n            stage1.trajectories[:size] = stage1.trajectories[ngrow:oldsize]\n            stage1.matrices[:size] = stage1.matrices[ngrow:oldsize]\n            stage1.scores[:size] = stage1.scores[ngrow:oldsize]\n            for attr in (\"covar\", \"residuals\", \"fragcoms\", \"all_matrices\"):\n                v = getattr(stage1, attr)\n                if isinstance(v, np.ndarray):\n                    v[:size] = v[ngrow:oldsize]\n            \n            coor = stage1.coor\n            if coor is not None:\n                for attr in dir(coor):\n                    v = getattr(coor, attr)\n                    if isinstance(v, np.ndarray):\n                        v[:size] = v[ngrow:oldsize]\n\ndef grow(stage1: Stage, stage2: Stage, *, scorer, updaters):\n    return _grow(stage1, stage2, scorer=scorer, updaters=updaters, random=False)\n\ndef grow_random(stage1: Stage, stage2: Stage, *, scorer, updaters):\n    return _grow(stage1, stage2, scorer=scorer, updaters=updaters, random=True)\n\ndef init_coor_backbone(ms, fraglen, bbsize):\n    for stage in ms.stages:\n        stage.coor.backbone = np.zeros((stage.maxsize,fraglen, bbsize, 3))\n\ndef sort_score(stage: Stage) -> None:\n    \"\"\"Sort the stage by score, lowest first\"\"\"\n    assert stage.size\n    size = stage.size\n    scores = stage.scores[:size]\n    ind = np.argsort(scores)\n    stage.trajectories[:size] = stage.trajectories[ind]\n    stage.matrices[:size] = stage.matrices[ind]\n    stage.scores[:size] = stage.scores[ind]\n    if stage.covar is not None:\n        stage.covar[:size] = stage.covar[ind]\n    if stage.residuals is not None:\n        stage.residuals[:size] = stage.residuals[ind]\n    if stage.fragcoms is not None:\n        stage.fragcoms[:size] = stage.fragcoms[ind]\n    if stage.all_matrices is not None:\n        stage.all_matrices[:size] = stage.all_matrices[ind]\n    if stage.coor is not None:\n        for attr in dir(stage.coor):\n            v = getattr(stage.coor, attr)\n            if v is not None:\n                v[:size] = v[ind]\n\ndef filter_score_sorted(stage: Stage) -> None:\n    \"\"\"Filters stage by score_threshold, if it has one\n    Assumes that the stage has been sorted by score\"\"\"\n    if stage.size and stage.score_threshold is not None:\n        pos = np.searchsorted(\n            stage.scores[:stage.size], \n            stage.score_threshold, \n            side='right'\n        )\n        if pos < stage.size:\n            stage.size = pos\n\ndef filter_score_unsorted(stage: Stage, old_size) -> None:\n    \"\"\"Filters new trajectories in stage by score_threshold, if it has one\n    Does not assume that the stage has been sorted by score\n    Trajectories beyond old_size are assumed to be new\"\"\"\n    if stage.size > old_size and stage.score_threshold is not None:\n        mask = (stage.scores[old_size:stage.size] <= stage.score_threshold)\n        masksum = mask.sum()\n        if masksum == stage.size - old_size: # Nothing to do\n            return\n        if masksum == 0:\n            stage.size = old_size\n            return\n        new_size = old_size + masksum\n\n        for attr in (\n            \"trajectories\", \"matrices\", \"scores\", \n            \"covar\", \"residuals\", \"fragcoms\",\n            \"all_matrices\"\n        ):\n            v = getattr(stage.coor, attr)\n            if v is not None:\n                v[old_size:new_size] = v[old_size:stage.size][mask]\n        if stage.coor is not None:\n            for attr in dir(stage.coor):\n                v = getattr(stage.coor, attr)\n                if v is not None:\n                    v[old_size:new_size] = v[old_size:stage.size][mask]\n        stage.size = new_size",
    "dependencies": [
      ".MainState",
      ".functions.matrix"
    ],
    "language": "python"
  },
  "progressions.kbest": {
    "code": "\"\"\"Progressions to enumerate the k-best fragment trajectories \n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nfrom ..MainState import MainState\nfrom .grow import grow, sort_score, filter_score_sorted, filter_score_unsorted\nimport numpy as np\nimport time\nimport logging\nlogger = logging.getLogger(\"nefertiti\")\n\nfrom typing import List\n\ndef kbest(\n    s: MainState,\n    scorer: callable,\n    updaters:list,\n    k,\n    *,\n    minblocksize:int,\n    upper_estimate:float,\n    downstream_best:List[float],\n    optblocksize=100,   # for stages at the end\n    optblocksize2=5,    # for stages at the end, k=1\n) -> None:\n\n    def report():\n        print(curr_time - start)\n        for n in range(nstages):\n            stage = s.stages[n]\n            score_threshold = 999\n            if n == nstages-1 and cutoff is not None:\n                score_threshold = cutoff\n            elif stage.score_threshold is not None:\n                score_threshold = stage.score_threshold\n            print(\"{:d} {:6d} {:.3f}\".format(n+1, stage.size, score_threshold))\n        print()\n\n    cutoff = None\n    start = time.time()\n    s1 = s.stages[0]\n    grow(None, s1, scorer=scorer, updaters=updaters)\n    sort_score(s1)\n\n    nstages = len(s.stages)\n    if nstages == 1:\n        return\n\n    has_new_cutoff = False\n    delta = 0.1\n    def propagate_threshold(stagenr, threshold, final_threshold):\n        nonlocal cutoff, has_new_cutoff\n        if stagenr == nstages - 1:\n            if cutoff is not None:\n                assert threshold <= cutoff\n                d = cutoff - threshold\n                old_cutoff = cutoff\n                cutoff = threshold\n                if d < delta:\n                    return\n                has_new_cutoff = True\n            else:\n                cutoff = threshold\n        else:\n            stage = s.stages[stagenr]\n            curr_threshold = stage.score_threshold\n            if curr_threshold is not None:\n                if threshold >= curr_threshold:\n                    return\n                d = curr_threshold - threshold\n                stage.score_threshold = threshold       \n                if d < delta:\n                    return\n            else:\n                stage.score_threshold = threshold\n            filter_score_sorted(stage)\n        if stagenr > 0:\n            next_threshold = final_threshold - downstream_best[stagenr-1]\n            propagate_threshold(stagenr-1, next_threshold, final_threshold)        \n\n    if upper_estimate is not None:\n        propagate_threshold(nstages-1, upper_estimate, upper_estimate)\n\n    last_time = start\n    while 1:\n        curr_time = time.time()\n        if curr_time - last_time > 5:\n            report()\n            last_time = curr_time\n        progress = False\n        for curr_minblocksize in (minblocksize, 1):\n            if curr_minblocksize == 1 and progress:\n                break\n            for n in range(nstages-2, -1, -1):\n                if n < nstages-2:\n                    s1, s2 = s.stages[n], s.stages[n+1]\n                    space = s2.maxsize - s2.size\n                    if space // s.fraglib.nfrags < curr_minblocksize:\n                        continue\n                if s.stages[n].size >= optblocksize:\n                    break\n                if k == 1 and not has_new_cutoff:\n                    if s.stages[n].size >= optblocksize2:\n                        break                    \n            else:\n                for n in range(0, nstages-1):\n                    if n < nstages-2:\n                        s1, s2 = s.stages[n], s.stages[n+1]\n                        space = s2.maxsize - s2.size\n                        if space // s.fraglib.nfrags < curr_minblocksize:\n                            continue\n                    if s.stages[n].size >= curr_minblocksize:\n                        break\n                else:\n                    continue\n\n            progress = True\n            s1, s2 = s.stages[n], s.stages[n+1]\n            old_size = s2.size\n            grow(s1, s2, scorer=scorer, updaters=updaters)\n            if n+1 == nstages-1:\n                filter_score_unsorted(s2, old_size)\n                if s2.size > old_size and s2.size >= k:\n                    sort_score(s2)              \n                    threshold = s2.scores[k-1]                    \n                    propagate_threshold(nstages-1, threshold, threshold)\n                    s2.size = k\n            else:\n                sort_score(s2)\n                filter_score_sorted(s2)            \n        if not progress:\n            break\n    s.downstream_best_score = [s.stages[-1].scores[0]] + downstream_best",
    "dependencies": [
      ".MainState",
      ".progressions.grow"
    ],
    "language": "python"
  },
  "progressions.prepare_backbone": {
    "code": "\"\"\" (description)\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nfrom ..MainState import StructureRepresentation\nfrom ..functions.prepare_backbone import prepare_backbone as prepare_backbone_func\nfrom ..functions.parse_pdb import parse_pdb, get_backbone, get_xyz, get_sequence\n\nimport numpy as np\n\ndef prepare_backbone_from_pdb(s:StructureRepresentation, pdb) -> None:\n    \"\"\"Updates `s` by parsing `pdb` and calling the prepare_backbone progression\n    `pdb` must be a file-like object or a text string in PDB format\n    Also sets s.coor to the result of the parse_pdb function\n    \"\"\"\n    if hasattr(pdb, \"read\"):\n        pdbdata = pdb.read()\n    else:\n        pdbdata = pdb\n    struc = parse_pdb(pdbdata)\n    if s.bb_atoms is None:\n        s.bb_atoms = [\"N\", \"CA\", \"C\", \"O\"]\n    bb = get_backbone(struc, s.bb_atoms)\n    bb_coor = get_xyz(bb)\n    s.coor = struc\n    s.sequence = get_sequence(struc)\n    prepare_backbone(s, bb_coor)\n\ndef prepare_backbone(s: StructureRepresentation, bb_coor:np.ndarray) -> None:\n    \"\"\"Updates `s` by calling the prepare_backbone function\"\"\"\n    if s.bb_atoms is None:\n        s.bb_atoms = [\"N\", \"CA\", \"C\", \"O\"]\n    if s.fraglen is None:\n        s.fraglen = 4\n    s.nresidues = len(bb_coor)\n    if s.nfrags is None:\n        s.nfrags = s.nresidues - s.fraglen + 1\n    s.coor_residue = {}\n    s.coor_residue.backbone =  bb_coor\n    prep = prepare_backbone_func(\n        bb_coor, s.fraglen, len(s.bb_atoms)\n    )\n    bb_coor4, bb_coor_frag, bb_residuals_frag, bb_com_frag = prep\n    s.coor_residue.backbone4 = bb_coor4\n    com = bb_coor4.reshape(-1, 4).mean(axis=0)\n    com[3] = 0\n    s.coor_residue.backbone4_centered = bb_coor4 - com\n    s.coor_fragment = {}\n    s.coor_fragment.backbone4_centered = bb_coor_frag\n    s.coor_fragment.backbone_residuals = bb_residuals_frag\n    s.coor_fragment.backbone_com = bb_com_frag\n\ndef select_last_backbone(s: StructureRepresentation, nfrags:int) -> None:\n    \"\"\"Updates `s` that has a prepared backbone, by selecting the last residues\"\"\"\n    nresidues = int(nfrags + s.fraglen - 1)\n    s.nresidues = nresidues\n    s.nfrags = nfrags\n    s.coor_residue.backbone =  s.coor_residue.backbone[-nresidues:]\n    prep = prepare_backbone_func(\n        s.coor_residue.backbone, s.fraglen, len(s.bb_atoms)\n    )\n    bb_coor4, bb_coor_frag, bb_residuals_frag, bb_com_frag = prep\n    s.coor_residue.backbone4 = bb_coor4\n    com = bb_coor4.reshape(-1, 4).mean(axis=0)\n    com[3] = 0\n    s.coor_residue.backbone4_centered = bb_coor4 - com\n    s.coor_fragment = {}\n    s.coor_fragment.backbone4_centered = bb_coor_frag\n    s.coor_fragment.backbone_residuals = bb_residuals_frag\n    s.coor_fragment.backbone_com = bb_com_frag    ",
    "dependencies": [
      ".MainState",
      ".functions.parse_pdb",
      ".functions.prepare_backbone"
    ],
    "language": "python"
  },
  "progressions.randombest": {
    "code": "\"\"\"Progressions to randomly generate fragment trajectories\nwithin an (optional) score threshold\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nfrom ..MainState import MainState\nfrom .grow import grow_random, sort_score, filter_score_unsorted\nimport numpy as np\nimport time\nimport logging\nlogger = logging.getLogger(\"nefertiti\")\n\nfrom typing import List\n\ndef randombest(\n    s: MainState,\n    scorer: callable, #or None\n    updaters:list,\n    *,\n    ntrajectories=int,\n    threshold:float, # or None\n    downstream_best:List[float], # or None\n    minblocksize:int,\n) -> None:\n\n    nstages = len(s.stages)\n    assert s.stages[-1].maxsize >= ntrajectories\n\n    if downstream_best is not None:\n        assert threshold is not None\n        assert len(downstream_best) == nstages - 1\n        t = threshold\n        for n in range(nstages-1, -1, -1):\n            stage = s.stages[n]\n            stage.score_threshold = t\n            t -= downstream_best[n-1]\n    else:\n        s.stages[-1].score_threshold = threshold\n        \n    def report():\n        print(curr_time - start)\n        for n in range(nstages):\n            stage = s.stages[n]\n            score_threshold = 999\n            if stage.score_threshold is not None:\n                score_threshold = stage.score_threshold\n            print(\"{:d} {:6d} {:.3f}\".format(n+1, stage.size, score_threshold))\n        print()\n\n    start = time.time()\n    last_time = start\n    first_stage = s.stages[0]\n    last_stage = s.stages[-1]\n    while last_stage.size < ntrajectories:\n\n        curr_time = time.time()\n        if curr_time - last_time > 5:\n            report()\n            last_time = curr_time\n        \n        if first_stage.size < minblocksize:\n            old_size = first_stage.size\n            progress = True\n            grow_random(None, first_stage, scorer=scorer, updaters=updaters)\n            filter_score_unsorted(first_stage, old_size)\n            continue\n\n        progress = False\n        for curr_minblocksize in (minblocksize, 1):\n            if curr_minblocksize == 1 and progress:\n                break\n            for n in range(nstages-2, -1, -1):\n                s1, s2 = s.stages[n], s.stages[n+1]\n                if s1.size < curr_minblocksize:\n                    continue\n                space = s2.maxsize - s2.size\n                if space // s.fraglib.nfrags < curr_minblocksize:\n                    continue\n                break\n            else:\n                continue\n\n            progress = True\n            old_size = s2.size\n            s1, s2 = s.stages[n], s.stages[n+1]\n            grow_random(s1, s2, scorer=scorer, updaters=updaters)\n            filter_score_unsorted(s2, old_size)\n    sort_score(s.stages[-1])",
    "dependencies": [
      ".MainState",
      ".progressions.grow"
    ],
    "language": "python"
  },
  "progressions.rmsd": {
    "code": "\"\"\"Progressions related to RMSD calculation\n\nAuthor: Sjoerd de Vries\nLicense: GPLv3, https://www.gnu.org/licenses/gpl-3.0.en.html\n\nThis file is part of the Nefertiti project.\n\"\"\"\n\nimport numpy as np\n\nfrom ..MainState import Stage\nfrom ..functions.superimpose import superimpose_array, superimpose_array_from_covar\n\ndef update_residuals_covar_backbone(\n    s: Stage, newsize\n) -> None:\n    size = s.size\n    coor = s.coor.backbone[size:newsize]\n    coor = coor.reshape(len(coor), -1, 3)\n    fragcom = s.matrices[size:newsize, 3,:3]\n    coor_c = coor - fragcom[:, None]\n        \n    refe = s.refe.coor_fragment.backbone4_centered\n    refe_coor = refe[s.fragindex-1].reshape(-1, 4)[:, :3]\n    \n    residuals = np.einsum(\"ijk,ijk->i\", coor_c, coor_c)\n    covar = np.einsum(\"ikj,kl->ijl\", coor_c, refe_coor)    \n    s.residuals[size:newsize] += residuals\n    s.covar[size:newsize] += covar\n    s.fragcoms[size:newsize, -1] = fragcom\n    \ndef rmsd_backbone(\n    s: Stage, newsize\n) -> None:\n    # TODO: optimize the code within this function, \n    #   as it becomes a speed bottleneck for long molecules\n    size = s.size\n\n    refe = s.refe.coor_fragment\n    refe_self_residual = refe.backbone_residuals[:s.fragindex].sum()\n    nfragatoms = s.fraglen * len(s.bb_atoms)\n    natoms = s.fragindex * nfragatoms\n\n    refe_shifts = refe.backbone_com[:s.fragindex]\n    refe_shifts = refe_shifts - refe_shifts.mean(axis=0)\n    refe_shift_residual = nfragatoms * (refe_shifts * refe_shifts).sum()\n    refe_residual = refe_self_residual + refe_shift_residual\n\n    coor_self_residuals = s.residuals[size:newsize]\n    coor_shifts = s.fragcoms[size:newsize]\n    coor_shifts = coor_shifts - coor_shifts.mean(axis=1)[:, None]\n    coor_shift_residuals = nfragatoms * np.einsum(\"ijk,ijk->i\",coor_shifts,coor_shifts)\n    coor_residuals = coor_self_residuals + coor_shift_residuals\n\n    covar_raw = s.covar[size:newsize]\n    covar_shift0 = coor_shifts[:, :, :, None] * refe_shifts[None, :, None, :]\n    covar_shift = covar_shift0.sum(axis=1) * nfragatoms \n    covar = covar_raw + covar_shift\n\n    residuals = coor_residuals + refe_residual\n    \n    sd = superimpose_array_from_covar(covar, residuals, natoms, return_sd=True)\n    s.scores[size:newsize] = sd\n\n    \"\"\"\n    # Check (up to first 2 fragments)\n    if s.fragindex <= 2:\n\n        rotmat, rmsd = superimpose_array_from_covar(covar, residuals, natoms, return_sd=False)\n        print(rotmat[0], rmsd[:10])\n\n        all_refe_coor = refe.coor_fragment.backbone4_centered\n        this_refe_coor = all_refe_coor[:s.fragindex].reshape(-1, 4)[:, :3]\n        this_refe_coor = this_refe_coor - this_refe_coor.mean(axis=0)\n\n        coor = s.coor.backbone[size:newsize]\n        coor = coor.reshape(len(coor), -1, 3)\n        if s.fragindex == 2:\n            fraglib = s.parent.fraglib\n            t = s.trajectories[size:newsize, 0]         \n            old_coor = fraglib.coor.backbone4_centered[t][:, :, :, :3]   \n            old_coor = old_coor.reshape(len(old_coor), -1, 3)\n            coor = np.concatenate((old_coor, coor), axis=1)\n        coor = coor - coor.mean(axis=1)[:, None] \n        rotmat_direct, rmsd_direct = superimpose_array(coor, this_refe_coor)\n        print(rotmat_direct[0], rmsd_direct[:10])\n    \"\"\"",
    "dependencies": [
      ".MainState",
      ".functions.superimpose"
    ],
    "language": "python"
  },
  "protocols.__init__": {
    "code": "",
    "dependencies": [],
    "language": "python"
  },
  "protocols.greedy": {
    "code": "import numpy as np\nfrom nefertiti.procedures.greedy import greedy_backbone_rmsd as _greedy_backbone_rmsd\n\ndef greedy_backbone_rmsd(\n    refe, #str | np.ndarray,\n    fraglib: np.ndarray,\n    *,\n    format: str,\n    poolsize: int\n) -> tuple:\n    \"\"\"\n    Builds near-native fragment trajectories in a greedy way\n    using a fragment-duplicated backbone RMSD\n\n    Input: \n    - a reference structure with nres residues.\n      \"format\" indicates the format.\n      - \"pdb\": in PDB format (text string)\n      - \"npy\": as a shape=(nresidues, len(bb_atoms), 3) numpy array\n    - a fragment library of shape (nfrag, fraglen, len(bb_atoms), 3)\n\n    poolsize: size of the greedy pool \n    (keep the best <poolsize> RMSDs at each trajectory growing stage)\n\n    Returns:\n    - trajectories, a (poolsize, nresidues-fraglen+1) array of np.uint32\n    - rmsds, a (poolsize) array of float\n    \"\"\"\n    if isinstance(refe, str) and len(refe) < 200:\n        raise ValueError(\"This function takes data content, not file names or URLs\")\n    main_state = _greedy_backbone_rmsd(\n        refe, fraglib,\n        format=format,\n        poolsize=poolsize\n    )\n    natoms = main_state.refe.nfrags * main_state.refe.fraglen * len(main_state.refe.bb_atoms)\n    last_stage = main_state.stages[-1]\n    trajectories = last_stage.trajectories[:poolsize]\n    scores = last_stage.scores[:poolsize]\n    rmsds = np.sqrt(scores/natoms)\n    return trajectories, rmsds",
    "dependencies": [
      ".procedures.greedy"
    ],
    "language": "python"
  },
  "protocols.kbest": {
    "code": "import numpy as np\nfrom nefertiti.procedures.kbest import kbest_backbone_rmsd as _kbest_backbone_rmsd\n\ndef kbest_backbone_rmsd(\n    refe, #str | np.ndarray,\n    fraglib: np.ndarray,\n    *,\n    format: str,\n    k: int\n) -> tuple:\n    \"\"\"\n    Enumerates the k-best near-native fragment trajectories\n    using a fragment-duplicated backbone RMSD\n    Input: \n    - a reference structure with nres residues.\n      \"format\" indicates the format.\n      - \"pdb\": in PDB format (text string)\n      - \"npy\": as a shape=(nresidues, len(bb_atoms), 3) numpy array\n    - a fragment library of shape (nfrag, fraglen, len(bb_atoms), 3)\n\n    k: the best trajectories to keep\n    Returns:\n    - trajectories, a (poolsize, nresidues-fraglen+1) array of np.uint32\n    - rmsds, a (poolsize) array of float\n    \"\"\"\n    if isinstance(refe, str) and len(refe) < 200:\n        raise ValueError(\"This function takes data content, not file names or URLs\")\n\n    main_state = _kbest_backbone_rmsd(\n        refe, fraglib,\n        format=format,\n        k=k\n    )\n    natoms = main_state.refe.nfrags * main_state.refe.fraglen * len(main_state.refe.bb_atoms)\n    last_stage = main_state.stages[-1]\n    trajectories = last_stage.trajectories[:k]\n    scores = last_stage.scores[:k]\n    rmsds = np.sqrt(scores/natoms)\n    return trajectories, rmsds",
    "dependencies": [
      ".procedures.kbest"
    ],
    "language": "python"
  },
  "protocols.randombest": {
    "code": "import numpy as np\nfrom nefertiti.procedures.randombest import randombest_backbone_rmsd as _randombest_backbone_rmsd\n\ndef randombest_backbone_rmsd(\n    refe, #str | np.ndarray,\n    fraglib: np.ndarray,\n    *,\n    format: str,\n    ntrajectories: int,\n    max_rmsd: float,\n    use_downstream_best: bool,\n    random_seed = 0,\n) -> tuple:\n    \"\"\"\n    Builds near-native fragment trajectories by random sampling\n    using a fragment-duplicated backbone RMSD\n\n    Input: \n    - a reference structure with nres residues.\n      \"format\" indicates the format.\n      - \"pdb\": in PDB format (text string)\n      - \"npy\": as a shape=(nresidues, len(bb_atoms), 3) numpy array\n    - a fragment library of shape (nfrag, fraglen, len(bb_atoms), 3)\n\n    ntrajectories: the number of trajectories to generate.\n\n    max_rmsd: maximum RMSD threshold for the generated trajectories\n\n    use_downstream_best: if true, first obtain the downstream best RMSDs\n     for fragment 2..N until the end. This will be very time-consuming for\n    large structures. However, this will then prune the search very \n    efficiently. Recommended if both the structure and the max_rmsd are\n    small.\n\n    Returns:\n    - trajectories, a (poolsize, nresidues-fraglen+1) array of np.uint32\n    - rmsds, a (poolsize) array of float\n    \"\"\"\n    if isinstance(refe, str) and len(refe) < 200:\n        raise ValueError(\"This function takes data content, not file names or URLs\")\n\n    main_state = _randombest_backbone_rmsd(\n        refe, fraglib,\n        format=format,\n        ntrajectories=ntrajectories,\n        max_rmsd=max_rmsd,\n        use_downstream_best=use_downstream_best,\n        random_seed=random_seed\n    )\n    natoms = main_state.refe.nfrags * main_state.refe.fraglen * len(main_state.refe.bb_atoms)\n    last_stage = main_state.stages[-1]\n    trajectories = last_stage.trajectories[:ntrajectories]\n    scores = last_stage.scores[:ntrajectories]\n    rmsds = np.sqrt(scores/natoms)\n    return trajectories, rmsds",
    "dependencies": [
      ".procedures.randombest"
    ],
    "language": "python"
  },
  "utils.__init__": {
    "code": "",
    "dependencies": [],
    "language": "python"
  },
  "utils.fit_nn": {
    "code": "import numpy as np\nimport scipy.stats\nimport matplotlib\nfrom io import BytesIO\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter, MaxNLocator\n\ndef size_to_logN2(size):\n    return np.log(size) ** 2\n\ndef logN2_to_size(logN2):\n    return np.exp(np.sqrt(logN2))\n\ndef format_fn(tick_val, tick_pos):\n    if tick_val < 0:\n        return ''\n    else:\n        size = logN2_to_size(tick_val)\n        if size < 100000:\n            return str(int(size))\n        else:    \n            s = (\"$%3.2e}$\" % size).replace(\"e+0\", \"e+\").replace(\"e+\", \"\\cdot 10^{\")\n            return s\n\ndef get_fitmarkers(x, y, param,binning_offset,log_binning_offset=0):\n    if param[\"mode\"] == \"lowest_point\":\n        return x[:1].copy(), y[:1].copy()\n    elif param[\"mode\"] == \"highest_point\":\n        return x[-1:].copy(), y[-1:].copy()\n    elif param[\"mode\"] == \"midpoint\":\n        p = int(len(x)/2)\n        return x[p:p+1].copy(), y[p:p+1].copy()\n    elif param[\"mode\"] == \"binning\":\n        nbins = param[\"bins\"]\n        values, bins = np.histogram(x,bins=nbins)\n        y = np.log(np.cumsum(values)+binning_offset) + log_binning_offset\n        return bins[1:].copy(), y**2\n    \ndef fit(x, y):\n    # or: linear regression, with no first-order term\n    b = 0\n    a, c, _, _, _ = scipy.stats.linregress(y, x)\n\n    p = 1/a\n    q = (b*b-4*a*c)/(4*a*a)\n    r = -b/(2*a)\n    return a,b,c, p,q,r\n\ndef fit_nn(parameters, data, nfraglib, fraglen):\n\n    nres = parameters[\"nresidues\"]\n    maxrmsd = 10\n    threshold = data.get(\"threshold\")\n    if threshold is not None:\n        maxrmsd = threshold\n\n    fig = plt.figure()\n    fig.set_figwidth(14*0.6)\n    fig.set_figheight(12*0.6)\n    ax = fig.add_subplot(111)\n\n    ax.yaxis.set_major_formatter(FuncFormatter(format_fn))\n    log_maxsize = np.log(nfraglib) * (nres - fraglen + 1)\n    ax.plot(\n        [0, 1000], [log_maxsize**2, log_maxsize**2], \n        color=\"red\",\n        linestyle=\"dashed\"\n    )\n    upper = log_maxsize**2 * 1.05\n    ax.set_ylim([0, upper])\n    oom = np.sqrt(upper)/np.log(10)\n    best = None\n    for inc in 1,2,5:\n        ticks1 = oom/inc\n        scale = 10**(np.floor(np.log(ticks1)/np.log(10)) - 1)\n        ticks2 = int(ticks1 / scale) + 1\n        if ticks2 >= 10:      \n            if best is None or ticks2 < best[0]:\n                best = ticks2, scale *  inc\n    yticks0 = np.arange(best[0]) * best[1]\n    if best[0] < 20:\n        skip = int(max(10 - 0.7 * (20-best[0]), 0))\n    else:\n        skip = int(0.5 * best[0])\n    yticks0 = yticks0[:1].tolist() + yticks0[skip:].tolist()\n    yticks = [(t*np.log(10))**2 for t in yticks0]\n    \n    pos = ax.get_position()\n    pos = [pos.x0 + 0.05, pos.y0+0.03,  pos.width, pos.height] \n    ax.set_position(pos)\n    ax.set_ylabel(\"Cumulative ensemble size\", size = \"x-large\")\n    ax.set_yticks(yticks)\n    ax.set_xlabel(\"RMSD (\u00c5)\", size=\"x-large\")\n\n    fit_points = []\n\n    phigh1 = parameters[\"high-rmsd\"][\"computation\"]\n    phigh2 = parameters[\"high-rmsd\"][\"analysis\"]\n    high_mode = phigh2[\"mode\"]\n    high_mode = high_mode.split(\",\")\n    if len(high_mode) == 1:\n        high_fit_mode = high_show_mode = high_mode[0]\n    else:\n        high_fit_mode, high_show_mode = high_mode\n\n    show_random = (high_fit_mode == \"random\"  or high_show_mode == \"random\")\n    random_rmsds = data.get(\"random\")\n    if random_rmsds is None:\n        show_random = False\n\n    show_threshold = (\n        high_fit_mode in (\"threshold\", \"random\") \n        or high_show_mode in (\"threshold\", \"random\") \n    )\n    if threshold is None:\n        show_threshold = False\n\n    if show_threshold:\n        best_of_factor = phigh1[\"threshold\"][\"best_of_factor\"]\n        tlog = log_maxsize - np.log(best_of_factor)\n        if show_random:\n            nstruc = phigh1[\"random\"][\"nstructures\"]\n            sample_log = np.log(best_of_factor) + np.log(nstruc) - log_maxsize\n            logn0 = np.log(np.arange(len(random_rmsds))+1)\n            logn = logn0 - sample_log\n            x, y = random_rmsds, logn**2\n            if high_fit_mode == \"random\":\n                discard_upper = phigh2[\"random\"].get(\"discard_upper\")\n                if discard_upper:\n                    discard_upper = min(discard_upper, len(x)-1)\n                    ax.plot(x[-discard_upper:], y[-discard_upper:], color=\"red\")\n                    x = x[:-discard_upper]\n                    y = y[:-discard_upper]\n                discard_lower = phigh2[\"random\"].get(\"discard_lower\")\n                if discard_lower:\n                    ax.plot(x[:discard_lower], y[:discard_lower], color=\"red\")\n                    discard_lower = min(discard_lower, len(x)-1)\n                    x = x[discard_lower:]\n                    y = y[discard_lower:]\n                xm, ym = get_fitmarkers(\n                    x, y, phigh2[\"random\"],\n                    discard_lower,\n                    -sample_log\n                )\n                size = 50 if len(xm) < 20 else 15\n                ax.scatter(\n                    xm, ym, color=\"black\", \n                    marker=\"x\", s=size,\n                )\n                fit_points.append((xm, ym))\n            \n            color = \"blue\" if high_fit_mode == \"random\" else \"purple\"\n            ax.plot(x, y, color=color)\n        color = \"blue\" if high_fit_mode == \"threshold\" else \"purple\"\n        ax.scatter([threshold], [tlog**2], color=color, marker=\"x\",s=150)\n        if high_fit_mode == \"threshold\":\n            fit_points.append(([threshold], [tlog**2]))\n    \n\n    xmin = None\n\n    plow1 = parameters[\"low-rmsd\"][\"computation\"]\n    plow2 = parameters[\"low-rmsd\"][\"analysis\"]\n    low_mode = plow2[\"mode\"]\n    low_mode = low_mode.split(\",\")\n    if len(low_mode) == 1:\n        low_fit_mode = low_show_mode = low_mode[0]\n    else:\n        low_fit_mode, low_show_mode = low_mode\n\n    show_nn_intercept = (\n        low_fit_mode in (\"nn\", \"nn_intercept\") or\n        low_show_mode in (\"nn\", \"nn_intercept\")\n    )\n    show_nn = (\n        low_fit_mode == \"nn\" or\n        low_show_mode == \"nn\"\n    )\n    if show_nn_intercept or show_nn:\n        nn_rmsds = data.get(\"near-native\")\n        if nn_rmsds is None:\n            show_nn = False\n            show_nn_intercept = False\n\n    if show_nn_intercept:\n        nn_intercept = nn_rmsds[0]\n        xmin = nn_intercept\n        color = \"blue\" if low_fit_mode == \"nn_intercept\" else \"green\"\n        ax.scatter(nn_intercept, 1, color=color, marker=\"x\", s=150,clip_on=False)\n        if low_fit_mode == \"nn_intercept\":\n            fit_points.append(([nn_intercept], [0]))\n\n\n    show_greedy = (low_fit_mode == \"greedy\"  or low_show_mode == \"greedy\")\n    greedy_rmsds = data.get(\"greedy\")\n    if greedy_rmsds is None:\n        show_greedy = False\n    if show_greedy:\n        logn = np.log(np.arange(len(greedy_rmsds))+1)\n        x, y = greedy_rmsds, logn**2\n        discard = plow2[\"greedy\"].get(\"discard\")\n        if discard:\n            if not show_nn:\n                ax.plot(x[:discard], y[:discard], color=\"red\")\n            discard = min(discard, len(x)-1)\n            x = x[discard:]\n            y = y[discard:]\n        else:\n            discard = 0\n        if low_fit_mode == \"greedy\":\n            color = \"blue\"\n        else:\n            color = \"purple\"\n        xm, ym = get_fitmarkers(\n            x, y, plow2[\"greedy\"], discard\n        )\n        if len(xm) > 1:\n            size = 50 if len(xm) < 20 else 10\n            if not show_nn:\n                ax.scatter(\n                    xm, ym, color=\"black\", \n                    marker=\"x\", s=size,\n                )\n            a,b,c,p,q,r = fit(xm, ym)\n            greedy_intercept = c\n        else:\n            greedy_intercept = xm[0]\n        if not show_nn:\n            ax.plot(x, y, color=color)\n        ax.scatter(\n            [greedy_intercept], [0], \n            color=color, marker=\"x\", s=150,\n            clip_on=False,\n        )\n        if low_fit_mode == \"greedy\":\n            fit_points.append(([greedy_intercept], [0]))\n\n        if xmin is None or xmin > greedy_rmsds[0]:\n            xmin = greedy_rmsds[0]\n    \n\n    if show_nn:\n        logn = np.log(np.arange(len(nn_rmsds))+1)\n        x, y = nn_rmsds, logn**2\n        if low_fit_mode == \"nn\" and len(nn_rmsds) > 1:\n            discard = plow2[\"near-native\"].get(\"discard\")\n            if discard:\n                ax.plot(x[:discard], y[:discard], color=\"red\")\n                discard = min(discard, len(x)-1)\n                x = x[discard:]\n                y = y[discard:]\n            else:\n                discard = 0\n            xm, ym = get_fitmarkers(\n                x, y, plow2[\"near-native\"],discard\n            )\n            size=150 if len(xm) == 1 else (50 if len(xm) < 20 else 15)\n            ax.scatter(\n                xm, ym, color=\"black\", \n                marker=\"x\", s=size,\n                clip_on=False\n            )\n            fit_points.append((xm, ym))\n            color = \"blue\"\n        else:\n            color = \"green\"\n        ax.plot(x, y, color=color)\n\n    result = {}\n    \n    if len(fit_points):\n        fitx = np.concatenate(\n            [p[0] for p in fit_points]\n        )\n        fity = np.concatenate(\n            [p[1] for p in fit_points]\n        )\n        assert len(fitx) == len(fity)\n        if len(fitx) > 1:\n            a,b,c,p,q,r = fit(fitx, fity)\n            fit_size = maxrmsd * p + q\n            ax.plot(\n                [c, maxrmsd], [0, fit_size],\n                color=\"black\",\n                linestyle=\"dotted\"\n            )\n            equation = {\n                \"a\": a,\n                \"b\": b,\n                \"c\": c,\n                \"p\": p,\n                \"q\": q,\n                \"r\": r,\n            }\n            text = \"log(N)\u00b2 = %.2f * RMSD + %.2f\" % (p,q)\n            text += \"\\n\"\n            text += \"RMSD = %.6e * log(N)\u00b2 + %.3f\" % (a,c)\n            equation[\"text\"] = text\n            result[\"equation\"] = equation\n\n    if xmin is None:\n        xmin = 0\n    ax.set_xlim(left=xmin*0.95, right=maxrmsd*1.05)\n\n    plotobj = BytesIO()\n    plt.savefig(plotobj)    \n    result[\"plot\"] = plotobj.getvalue()\n    return result\n\nif __name__ == \"transformer\":\n    from silk import Silk\n    if isinstance(data, Silk):\n        data = data.unsilk\n    if isinstance(parameters, Silk):\n        parameters = parameters.unsilk\n    result = fit_nn(parameters, data, nfraglib, fraglen)\n    result[\"plot\"] = np.array(result[\"plot\"]) #BUG in seamless",
    "dependencies": [],
    "language": "python"
  },
  "utils.nn_cost": {
    "code": "from seamless.core.transformation import SeamlessTransformationError\n\n\ndef nn_cost(parameters):\n    \"\"\"Returns if the cost is too high, plus a reason\"\"\"\n    nres = parameters[\"nresidues\"]\n    \n    if nres > 1000:\n        return True, \"Protein is too big\"\n\n    phigh1 = parameters[\"high-rmsd\"][\"computation\"]\n    phigh2 = parameters[\"high-rmsd\"][\"analysis\"]\n    high_mode = phigh2[\"mode\"]\n    high_mode = high_mode.split(\",\")\n    if len(high_mode) == 1:\n        high_fit_mode = high_show_mode = high_mode[0]\n    else:\n        high_fit_mode, high_show_mode = high_mode\n\n\n    if nres > 250:\n        if high_fit_mode == \"threshold\" or high_show_mode == \"threshold\":\n            return True, \"Too large for threshold calculation\" #  until memory issue solved\n    if nres > 120:\n        if high_fit_mode == \"random\" or high_show_mode == \"random\":\n            return True, \"Too large for random best\" #  until memory issue solved\n    \n    if high_fit_mode in (\"threshold\", \"random\") or high_show_mode in (\"threshold\", \"random\"):\n        nstruc = phigh1[\"random\"][\"nstructures\"]\n        if nstruc > 1000:\n            return True, \"More than 1000 random structures\" # for now#\n        best_of_factor = phigh1[\"threshold\"][\"best_of_factor\"]\n        nsample1 = best_of_factor * phigh1[\"threshold\"][\"redundancy\"]   \n        if nres > 12:\n            if nsample1 > 20000:\n                return True, \"Too many samples for threshold detection\"\n        else:\n            if nsample1 > 200000:\n                return True, \"Too many samples for threshold detection\"\n        if high_fit_mode == \"random\" or high_show_mode == \"random\":\n            nsample2 = best_of_factor * nstruc\n            if nres > 12:\n                if nsample2 > 100000:\n                    return True, \"Too many samples for random best\"\n            else:\n                if nsample2 > 1000000:\n                    return True, \"Too many samples for random best\"\n\n    plow1 = parameters[\"low-rmsd\"][\"computation\"]\n    plow2 = parameters[\"low-rmsd\"][\"analysis\"]\n    low_mode = plow2[\"mode\"]\n    low_mode = low_mode.split(\",\")\n    if len(low_mode) == 1:\n        low_fit_mode = low_show_mode = low_mode[0]\n    else:\n        low_fit_mode, low_show_mode = low_mode\n    \n    if low_fit_mode == \"greedy\" or low_show_mode == \"greedy\":\n        poolsize = plow1[\"greedy\"][\"poolsize\"]\n        if poolsize > 1000:\n            return True, \"Greedy poolsize too large\"\n        if nres > 500 and poolsize > 100:\n            return True, \"Greedy poolsize too large for this protein size\"\n        if nres > 100 and poolsize > 200:\n            return True, \"Greedy poolsize too large for this protein size\"\n        if nres > 12 and poolsize > 500:\n            return True, \"Greedy poolsize too large for this protein/peptide size\"\n\n    if low_fit_mode == \"nn\" or low_show_mode == \"nn\":\n        k = plow1[\"near-native\"][\"k\"]\n        if nres > 20:\n            return True, \"Maximum peptide size 20 for near-native ensemble calculation\"\n        if nres > 12 and k > 1:\n            return True, \"Only k=1 for peptides above size 12 for near-native ensemble calculation\"\n        if nres > 8 and k > 10000:\n            return True, \"k too big for near-native ensemble\"\n        if k > 100000:\n            return True, \"k too big for near-native ensemble\"\n\n    return False, None\n\nif __name__ == \"transformer\":\n    from silk import Silk\n    if isinstance(parameters, Silk):\n        parameters = parameters.unsilk\n    has_err, reason = nn_cost(parameters)\n    if has_err:\n        raise SeamlessTransformationError(reason)\n    result = parameters",
    "dependencies": [],
    "language": "python"
  }
}
